{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjJn2R-Ft8Gi"
   },
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: __Shaan Hossain, Jinesh Shailesh Mehta__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO List and Planning:\n",
    "\n",
    "Last Updated: 9th Nov\n",
    "\n",
    "Tasks:\n",
    "- \n",
    "- Update the analysis answer for different generated graphs (embeddings and word clusters 2D)\n",
    "\n",
    "Shaan:\n",
    "- Update the answer for text normalization and pre-processing\n",
    "\n",
    "Jinesh:\n",
    "- Update the answer for similar and constrasts for the two selected dataset\n",
    "\n",
    "For next time:\n",
    "\n",
    "- Analysis on the images\n",
    "- Training feed forward neural network\n",
    "- Generation of sentences and comparing sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwlPkjgt8Gq"
   },
   "source": [
    "Step 1: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "(describe the provided dataset that you have chosen here)\n",
    "\n",
    "__From the given two choices, we have [Spooky Authors Dataset](https://www.kaggle.com/c/spooky-author-identification).__\n",
    "\n",
    "Describe what data set you have chosen to compare and contrast with the your chosen provided dataset. Make sure to describe where it comes from and it's general properties.\n",
    "\n",
    "(describe your dataset here)\n",
    "<b>\n",
    "- Our selected data: [Coronavirus tweets NLP](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification)\n",
    "- Description : The tweets have been pulled from Twitter and manual tagging has been done then. The names and usernames have been given codes to avoid any privacy concerns.\n",
    "- Columns: \n",
    "    - UserName\n",
    "    - ScreenName\n",
    "    - Location\n",
    "    - TweetAt\n",
    "    - OriginalTweet\n",
    "    - Sentiment\n",
    "- Similarities with chosen provided dataset: TODO: Update similarities\n",
    "- Constrasts with chosen provided dataset: TODO: Update constrasts</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    },
    "id": "uQLg8dGdt8Gr"
   },
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "from typing import List, Dict\n",
    "# libs used for preprocessing\n",
    "from gensim.parsing.preprocessing import stem_text, remove_stopwords, strip_punctuation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import ngrams\n",
    "# libs used for file reading and parsing\n",
    "from csv import reader\n",
    "# libs used for plotting\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "# libs used for generating word2vec models\n",
    "from gensim.models import Word2Vec\n",
    "# to compute the training using multiple threads\n",
    "from multiprocessing import cpu_count\n",
    "# other utilities\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from os import path\n",
    "# libs used for validating the word vectors and loading word vectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "# import keras\n",
    "# import numpy \n",
    "# import pandas\n",
    "# import scipy\n",
    "# import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj0A0mCkt8Gt"
   },
   "source": [
    "### a) Train embeddings on GIVEN dataset : Spooky Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Function to parse data from training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:25.438770Z",
     "start_time": "2020-10-24T04:39:24.888507Z"
    },
    "id": "x2IJbX_Mt8Gu"
   },
   "outputs": [],
   "source": [
    "# Read the file and prepare the training data \n",
    "# so that it is in the following format\n",
    "\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "def parse_data(training_file_path: str, percentage: int, select_column:int) -> List[str]:\n",
    "  \"\"\"This function is used to parse input lines\n",
    "  and returns a the provided percent of data.\n",
    "\n",
    "  Args:\n",
    "      lines (List[str]): list of lines\n",
    "      percentage (int): percent of the dataset needed\n",
    "      select_column (int): column to be selected from the dataset\n",
    "  Returns:\n",
    "      List[str]: lines (percentage of dataset)\n",
    "  \"\"\"\n",
    "  sentences = []\n",
    "  percentage_sentences = []\n",
    "  with open(training_file_path, \"r\", encoding=\"utf8\", errors=\"ignore\") as csvfile:\n",
    "    csv_reader = reader(csvfile)\n",
    "    #skipping header\n",
    "    header = next(csv_reader)\n",
    "\n",
    "    # line_length = len(list(csv_reader_copy))\n",
    "   \n",
    "    if header != None:\n",
    "      for row in csv_reader:\n",
    "        sentences.append(row[select_column])\n",
    "\n",
    "    end_of_data = int(len(sentences) * percentage * .01)\n",
    "    percentage_sentences = sentences[0:end_of_data]\n",
    "\n",
    "  return percentage_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Function to do preprocessing for the parsed data.\n",
    "    - Lower word case\n",
    "    - Remove stopwords\n",
    "    - Remove punctuations\n",
    "    - Tokenize line and add sentence seperator tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(running_lines: List[str]) -> List[List[str]]:\n",
    "  \"\"\"This function takes in the running test and return back the\n",
    "  preprocessed text. Four tasks are done as part of this:\n",
    "    1. lower word case\n",
    "    2. remove stopwords\n",
    "    3. remove punctuation\n",
    "    4. Add <s> and </s> for every sentence\n",
    "\n",
    "  Args:\n",
    "      running_lines (List[str]): list of lines\n",
    "\n",
    "  Returns:\n",
    "      List[List[str]]: list of sentences where each sentence is broken\n",
    "                        into list of words.\n",
    "  \"\"\"\n",
    "  preprocessed_lines = []\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "  for line in running_lines:\n",
    "    lower_case_data = line.lower()\n",
    "    data_without_stop_word = remove_stopwords(lower_case_data)\n",
    "    data_without_punct = strip_punctuation(data_without_stop_word)\n",
    "    processed_data = tokenizer.tokenize(data_without_punct)\n",
    "    processed_data.insert(0,\"<s>\")\n",
    "    processed_data.append(\"</s>\")\n",
    "    preprocessed_lines.append(processed_data)\n",
    "  return preprocessed_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Computing the parsing and preprocessing for both the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse and preprocess provided data\n",
    "print(\"Tokenizing Provided Dataset: Spooky Dataset\\n\")\n",
    "spooky_sentences = preprocessing(parse_data(\"data/provided/train/train.csv\", 100, 1))\n",
    "for token in spooky_sentences:\n",
    "  print(token)\n",
    "\n",
    "#parse and preprocess custom data\n",
    "print(\"\\nTokenizing Custom Dataset: Corona Tweets\\n\")\n",
    "covid_sentences = preprocessing(parse_data(\"data/custom/train/Corona_NLP_train.csv\", 100, 4))\n",
    "for token in covid_sentences:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Provide functionality for generating word embeddings for a given model.\n",
    "\n",
    "__Note: For the Word2Vec model training, we are using Skip Gram model as shown with value sg in the below code. Also, we are keep generating the vocab in sorted order (descending).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    },
    "id": "Od_L53GEt8Gv"
   },
   "outputs": [],
   "source": [
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "sg = 1 # The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "window = 5\n",
    "size = EMBEDDINGS_SIZE\n",
    "min_count = 1\n",
    "workers = cpu_count()\n",
    "sorted_vocab = 1 #1 for descending order\n",
    "\n",
    "def generate_embeddings(model_name: str, sentences: List[List[str]]) -> (str, str):\n",
    "    \"\"\"This function is used to generate embeddings (model and word vectors) for a given\n",
    "    model name and provided sentences.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): name of the model\n",
    "        sentences (List[List[str]]): sentences to be used for model creation\n",
    "\n",
    "    Returns:\n",
    "        str, str : model path and word vector path\n",
    "    \"\"\"\n",
    "    #generate word2vec model\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences, \n",
    "        vector_size=size, \n",
    "        window=window, \n",
    "        min_count=min_count, \n",
    "        workers=workers, \n",
    "        sg=sg,\n",
    "        sorted_vocab=sorted_vocab)\n",
    "    # save model\n",
    "    model_file_name = f'word2vec.{model_name}.model'\n",
    "    model_path = f'{model_name}/model'\n",
    "    # make sure all the parent folders exists \n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "    model_file_path = f'{model_path}/{model_file_name}'\n",
    "    model.save(model_file_path)\n",
    "    # Store just the words + their trained embeddings.\n",
    "    word_vectors_file_name = f'word2vec.{model_name}.wordvectors'\n",
    "    word_vectors = model.wv\n",
    "    word_vectors_path = f'{model_name}/wordvectors'\n",
    "    # make sure all the parent folders exists \n",
    "    Path(word_vectors_path).mkdir(parents=True, exist_ok=True)\n",
    "    word_vectors_file_path = f'{word_vectors_path}/{word_vectors_file_name}'\n",
    "    word_vectors.save(word_vectors_file_path)\n",
    "    return model_file_path, word_vectors_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Generate model and word vectors for Spooky dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_model_path, spooky_word_vectors_path = generate_embeddings(\"spooky\", spooky_sentences)\n",
    "# enable below code to see the word vector of 'process' word in spooky\n",
    "\n",
    "# spooky_wv = KeyedVectors.load(spooky_word_vectors_path, mmap='r')\n",
    "# process_word_vector = spooky_wv['process']\n",
    "# print(process_word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Display the vocabulary size for Spooky and Covid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:43.448249Z",
     "start_time": "2020-10-24T04:39:43.444835Z"
    },
    "id": "xrt52ahnt8Gw"
   },
   "outputs": [],
   "source": [
    "# print out the vocabulary size for spooky dataset\n",
    "spooky_model = Word2Vec.load(spooky_model_path)\n",
    "print('Vocab size: {} for Spooky Dataset.'.format(len(spooky_model.wv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Saving the word embeddings in a text file (load later if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    },
    "id": "UUanXgQLt8Gy"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "# make sure all the parent folders exists \n",
    "spooky_embeddings_folder_path = 'spooky/embeddings'\n",
    "Path(spooky_embeddings_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "spooky_model.wv.save_word2vec_format(f'{spooky_embeddings_folder_path}/spooky_embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGMUTMcmt8G0"
   },
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN0KvmUKt8G0"
   },
   "outputs": [],
   "source": [
    "# then do a second data set\n",
    "\n",
    "#generate model and word vectors for covid dataset\n",
    "covid_model_path, covid_word_vectors_path = generate_embeddings(\"covid\", covid_sentences)\n",
    "\n",
    "#enable below code to see the word vector of 'neighbours' word in covid\n",
    "# covid_wv = KeyedVectors.load(covid_word_vectors_path, mmap='r')\n",
    "# neighbours_word_vector = covid_wv['neighbours']\n",
    "# print(neighbours_word_vector)\n",
    "\n",
    "#print out the vocabulary size for covid dataset\n",
    "covid_model = Word2Vec.load(covid_model_path)\n",
    "print('Vocab size: {} for Covid Dataset.'.format(len(covid_model.wv)))\n",
    "\n",
    "# save embeddings\n",
    "covid_embeddings_folder_path = 'covid/embeddings'\n",
    "Path(covid_embeddings_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "covid_model.wv.save_word2vec_format(f'{covid_embeddings_folder_path}/covid_embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsjzTVFjt8G1"
   },
   "source": [
    "What text-normalization and pre-processing did you do and why?\n",
    "\n",
    "- Text Normalization and Pre-processing steps done:\n",
    "    - Lower word case\n",
    "    - Remove stopwords\n",
    "    - Remove punctuations\n",
    "    - Tokenize line and add sentence seperator tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOFmHpH8t8G2"
   },
   "source": [
    "Step 2: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce any graphs or figures that illustrate what you have found and write 2 - 3 paragraphs describing the differences you find between your two sets of embeddings and why you see them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Get top k frequent words from the parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(sentences:List[List[str]]):\n",
    "    \"\"\"This function is used to generate vocabs from the given sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): sentences\n",
    "\n",
    "    Returns:\n",
    "        dict: word with each having count\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        for i in sentence:\n",
    "            word_counts[i] += 1\n",
    "    return word_counts\n",
    "\n",
    "def get_top_k_words(sentences:str, k:int):\n",
    "    \"\"\"This function is used to get top k word from the given\n",
    "    sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (str): list of all the sentences to be used for count\n",
    "        k (int): top k words (max frequency)\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: k words with their count\n",
    "    \"\"\"\n",
    "    word_counts = generate_vocab(sentences)\n",
    "    word_counts[\"<s>\"] = 0\n",
    "    word_counts[\"</s>\"] = 0\n",
    "    k_most_common = []\n",
    "    for i in word_counts.most_common(k):\n",
    "        k_most_common.append(i[0])\n",
    "    print(word_counts.most_common(k)) #Seeing the corresponding counts\n",
    "    return k_most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate keys for both the datasets: Spooky and Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXjy2-OqgvIf"
   },
   "outputs": [],
   "source": [
    "# for spooky dataset\n",
    "wv_spooky = KeyedVectors.load(spooky_word_vectors_path, mmap='r')\n",
    "spooky_keys  = get_top_k_words(spooky_sentences, 10)\n",
    "# print(spooky_keys) \n",
    "# print(spooky_model.wv.most_similar(spooky_keys[0], topn=10))\n",
    "\n",
    "# for covid dataset\n",
    "wv_covid = KeyedVectors.load(covid_word_vectors_path, mmap='r')\n",
    "covid_keys  = get_top_k_words(covid_sentences, 10)\n",
    "# print(spooky_keys) \n",
    "# print(model.wv.most_similar(keys[0], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define embedding clusters and word clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_word_clusters(model, keys):\n",
    "    \"\"\"This function is used to generate embedding and word clusters\n",
    "    for the given model and the respective keys.\n",
    "\n",
    "    Args:\n",
    "        model (word2vec): model to be used\n",
    "        keys (Dict): keys which are most frequent in the model\n",
    "\n",
    "    Returns:\n",
    "        List, List: embedding and word cluster\n",
    "    \"\"\"\n",
    "    embedding_clusters = []\n",
    "    word_clusters = []\n",
    "    for word in keys:\n",
    "        embeddings = []\n",
    "        words = []\n",
    "        # print(model.wv.most_similar(word, topn=10))\n",
    "        for similar_word, _ in model.wv.most_similar(word, topn=10):\n",
    "            words.append(similar_word)\n",
    "            embeddings.append(model.wv[similar_word])\n",
    "        embedding_clusters.append(embeddings)\n",
    "        word_clusters.append(words)\n",
    "    return embedding_clusters, word_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define t-sne projection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, perplexity, filename=None):\n",
    "    embedding_clusters = np.array(embedding_clusters)\n",
    "    n, m, k = embedding_clusters.shape\n",
    "    tsne_model_en_2d = TSNE(perplexity=perplexity, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "    embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embeddings_en_2d, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Generate embedding and word clusters. Also, generate 2d projections for spooky and covid dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 100\n",
    "\n",
    "#spooky\n",
    "spooky_embeddings_clusters, spooky_word_clusters = generate_embeddings_word_clusters(spooky_model, spooky_keys)\n",
    "# for i in range(1, 30, 3):\n",
    "#     file_name = 'similar_words_perplexity_' + str(i) + '.png'\n",
    "#     tsne_plot_similar_words('Similar words from Spooky Dataset', keys, embedding_clusters, word_clusters, 0.7, i,\n",
    "#     file_name)\n",
    "spooky_folder_path = \"spooky/images\"\n",
    "Path(spooky_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "spooky_file_path = f'{spooky_folder_path}/similar_words_perplexity_{perplexity}.png'\n",
    "tsne_plot_similar_words('Similar words from Spooky Dataset', spooky_keys, spooky_embeddings_clusters, spooky_word_clusters, 0.7, perplexity,\n",
    "    spooky_file_path)\n",
    "\n",
    "# #covid\n",
    "covid_embeddings_clusters, covid_word_clusters = generate_embeddings_word_clusters(covid_model, covid_keys)\n",
    "covid_folder_path = \"covid/images\"\n",
    "Path(covid_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "covid_file_path = f'{covid_folder_path}/similar_words_perplexity_{perplexity}.png'\n",
    "tsne_plot_similar_words('Similar words from Covid Dataset', covid_keys, covid_embeddings_clusters, covid_word_clusters, 0.7, perplexity,\n",
    "    covid_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWlWydbrgv4P"
   },
   "source": [
    "#### 7. Produce any graphs or figures that illustrate what you have found and write 2 - 3 paragraphs describing the differences you find between your two sets of embeddings and why you see them.\n",
    "(Write down your analysis)\n",
    "\n",
    "Ans: TODO - Update the analysis answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tmrTVDqt8G2"
   },
   "source": [
    "Cite your sources:\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix2On6zat8G2"
   },
   "source": [
    "Step 3: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZsCKQWDt8G2"
   },
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define a function that would convert text to seqeunces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(tokenized_sentences:List[List[str]], vocab_index_mapping:dict) -> List[List[int]]:\n",
    "    \"\"\"This function is used to generate sequences from the given texts.\n",
    "\n",
    "    Args:\n",
    "        tokenized_sentences (List[List[str]]): list of list of tokens (all sentences)\n",
    "        vocab_index_mapping (dict): vocab mapped to index\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: list of encoded sentences\n",
    "    \"\"\"\n",
    "    encoded_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        encoded_sentence = []\n",
    "        for token in sentence:\n",
    "            encoded_sentence.append(vocab_index_mapping[token])\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "    return encoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate Spooky and Covid encoded sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    },
    "id": "ec0KKYj0t8G3"
   },
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# The size of the ngram language model you want to train\n",
    "# change as needed for your experiments\n",
    "NGRAM = 3 \n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "spooky_vocab = list(generate_vocab(spooky_sentences).keys())\n",
    "spooky_vocab_index_mapping = {word:index for index, word in enumerate(spooky_vocab)}\n",
    "spooky_encoded_sentences = texts_to_sequences(spooky_sentences, spooky_vocab_index_mapping)\n",
    "# for sentence in spooky_encoded_sentences:\n",
    "#     print(sentence)\n",
    "\n",
    "covid_vocab = list(generate_vocab(covid_sentences).keys())\n",
    "covid_vocab_index_mapping = {word:index for index, word in enumerate(covid_vocab)}\n",
    "covid_encoded_sentences = texts_to_sequences(covid_sentences, covid_vocab_index_mapping)\n",
    "# for sentence in covid_encoded_sentences:\n",
    "#     print(sentence)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCndArPmt8G5"
   },
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jG42_9Xt8G6"
   },
   "source": [
    "#### Fixed ngram based sequences "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HsoPVS8ct8G7"
   },
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process                                  however\n",
    "    process, however                                  afforded\n",
    "    however, afforded\t                              me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define function to generate ngrams from training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    },
    "id": "B_4YqhKTt8G7"
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded_sentences: list) -> list:\n",
    "    \"\"\"Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "\n",
    "    Args:\n",
    "        encoded_sentences (list): encoded sentences\n",
    "\n",
    "    Returns:\n",
    "        list: generated ngrams from encoded sentences \n",
    "        - list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    \"\"\"    \n",
    "    generated_ngrams_list = []\n",
    "    for sentence in encoded_sentences:\n",
    "        generated_ngrams_list.append(list(ngrams(sentence, NGRAM + 1)))\n",
    "\n",
    "    #enable to print generated ngrams\n",
    "    # for generated_ngrams in generated_ngrams_list:\n",
    "    #     for generated_ngram in generated_ngrams:\n",
    "    #         print(generated_ngram)\n",
    "\n",
    "    return generated_ngrams_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generating ngrams for spooky and covid encoded sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_ngrams = generate_ngram_training_samples(spooky_encoded_sentences)\n",
    "covid_ngrams = generate_ngram_training_samples(covid_encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWL6Czlxt8G8"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Defining function to split ngrams into X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:31.213422Z",
     "start_time": "2020-10-24T05:21:31.061759Z"
    },
    "id": "csweN-d1t8G9"
   },
   "outputs": [],
   "source": [
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]\n",
    "\n",
    "def split_ngram_to_training_sample(generated_ngrams: list):\n",
    "    \"\"\"This function is used to split the provided n grams into X and Y.\n",
    "\n",
    "    Args:\n",
    "        generated_ngrams (list): ngrams to be splitted\n",
    "\n",
    "    Returns:\n",
    "        X, Y: List X, List Y\n",
    "    \"\"\"\n",
    "    generated_n_grams_copy = generated_ngrams.copy()\n",
    "    X = []\n",
    "    Y = []\n",
    "    for ngrams in generated_n_grams_copy:\n",
    "        for ngram in ngrams:\n",
    "            ngram = list(ngram)\n",
    "            Y.append(ngram.pop(len(ngram) - 1))\n",
    "            X.append(ngram)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Generating X and Y from generated ngrams for spooky and covid datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_X, spooky_Y = split_ngram_to_training_sample(spooky_ngrams)\n",
    "# for i in range(0, len(spooky_X)):\n",
    "#     print(\"X: \" + str(spooky_X[i]))\n",
    "#     print(\"Y: \" + str(spooky_Y[i]))\n",
    "\n",
    "# print(spooky_X)\n",
    "# print(spooky_Y)\n",
    "\n",
    "covid_X, covid_Y = split_ngram_to_training_sample(covid_ngrams)\n",
    "# for i in range(0, len(covid_X)):\n",
    "#     print(\"X: \" + str(covid_X[i]))\n",
    "#     print(\"Y: \" + str(covid_X[i]))\n",
    "\n",
    "# print(covid_X)\n",
    "# print(covid_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Defining a function to convert X and Y from ngrams representation to embedddings representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    },
    "id": "Vjr6vwP5t8G9"
   },
   "outputs": [],
   "source": [
    "def convert_ngrams_to_embeddings(X_ngrams :List, Y_grams:List, vocabulary_index:Dict, word_embeddings:Dict):\n",
    "    \"\"\"This function is used to convert the provided ngrams into encoding representation \n",
    "    and for that, use the already created vocabulary_index dict and word embedding mappings.\n",
    "\n",
    "    Args:\n",
    "        X_ngrams (List): list of X ngrams\n",
    "        Y_grams (List): list of Y ngrams\n",
    "        vocabulary_index (Dict): vocab mapped to index\n",
    "        word_embeddings (Dict): word mapped to embedding\n",
    "\n",
    "    Returns:\n",
    "        X, Y: X and Y (having embedding representation)\n",
    "    \"\"\"\n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':[0....], ...}\n",
    "    # index to embedding : {1:[0....], ...} \n",
    "    # use your tokenizer's word_index to find the index of\n",
    "    # a given word\n",
    "    \n",
    "    #For X\n",
    "    X_sentences_embeddings = []\n",
    "    for items in X_ngrams:\n",
    "        X_sentence_embeddings = []\n",
    "        for index in items:\n",
    "            word = vocabulary_index[index]\n",
    "            embeddings = word_embeddings[word]\n",
    "            X_sentence_embeddings.extend(embeddings)\n",
    "        X_sentences_embeddings.append(X_sentence_embeddings)\n",
    "        \n",
    "    #For Y       \n",
    "    Y_total_embeddings = to_categorical(Y_grams)\n",
    "            \n",
    "    return X_sentences_embeddings, Y_total_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Converting X and Y from ngrams representation to embedddings representation from Spooky and Covid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_X_embeddings, spooky_Y_embeddings = convert_ngrams_to_embeddings(spooky_X, spooky_Y,spooky_vocab,wv_spooky)\n",
    "\n",
    "covid_X_embeddings, covid_Y_embeddings = convert_ngrams_to_embeddings(covid_X, covid_Y, covid_vocab, wv_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Defining a data generator for given X, Y and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    },
    "id": "H6g9g7p6t8G9"
   },
   "outputs": [],
   "source": [
    "def data_generator(X_embeddings: list, y_embeddings: list, num_sequences_per_batch: int) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    steps_per_epochs = len(X_embeddings)/num_sequences_per_batch\n",
    "    current_step = 0\n",
    "    while current_step<steps_per_epochs:\n",
    "        start_cell = (current_step) * num_sequences_per_batch\n",
    "        end_cell = (current_step+1) * num_sequences_per_batch\n",
    "        yield np.array(X_embeddings[start_cell:end_cell]),np.array(y_embeddings[start_cell:end_cell])\n",
    "        current_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Creating a data generator for given X, Y and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "# initialize data_generator\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical\n",
    "\n",
    "spooky_generator = data_generator(spooky_X_embeddings, spooky_Y_embeddings ,num_sequences_per_batch)\n",
    "spooky_sample = next(spooky_generator)\n",
    "print(spooky_sample[0].shape)\n",
    "print(spooky_sample[1].shape)\n",
    "\n",
    "covid_generator = data_generator(covid_X_embeddings, covid_Y_embeddings ,num_sequences_per_batch)\n",
    "# covid_sample = next(covid_generator)\n",
    "# print(covid_sample[0].shape)\n",
    "# print(covid_sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzfweqz1t8G-"
   },
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    },
    "id": "KmgNnQj5t8G_"
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "\n",
    "# Define the model architecture using Keras Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "id": "-dWf2qO3t8G_"
   },
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "model.fit(x=train_generator, \n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QwRhKYwt8HA"
   },
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    },
    "id": "ewR5ueOJt8HB"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list, \n",
    "                 n_words: int):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w68JVS2jt8HB"
   },
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE4dcQdut8HC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yet5p8N1t8HC"
   },
   "source": [
    "Sources Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sPu_1h2t8HC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wordembeddings_starter.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
