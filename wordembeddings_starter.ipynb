{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjJn2R-Ft8Gi"
   },
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: __Shaan Hossain, Jinesh Shailesh Mehta__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwlPkjgt8Gq"
   },
   "source": [
    "Step 1: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "(describe the provided dataset that you have chosen here)\n",
    "\n",
    "Describe what data set you have chosen to compare and contrast with the your chosen provided dataset. Make sure to describe where it comes from and it's general properties.\n",
    "\n",
    "(describe your dataset here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    },
    "id": "uQLg8dGdt8Gr"
   },
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "\n",
    "from typing import List\n",
    "from gensim.parsing.preprocessing import stem_text, remove_stopwords, strip_punctuation\n",
    "# import keras\n",
    "# import numpy \n",
    "# import pandas\n",
    "import scipy\n",
    "import sklearn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import ngrams\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from csv import reader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import KeyedVectors\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj0A0mCkt8Gt"
   },
   "source": [
    "### a) Train embeddings on GIVEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO: Jinesh\\n1. Parsing the custom dataset\\n2. Pre processing function - Completed.\\n\\nparse_data -> preprocessing ->\\n\\nTODO: Shaan\\n1. Get the infrastructure for the model set up to accept data from preprocessing\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO: Questions to submit\n",
    "1. Do we need to worry about the training time? Can we submit a pretrained model\n",
    "or is part of training what you want to see.\n",
    "2. Do we need to train on both of the datasets\n",
    "'''\n",
    "\n",
    "'''\n",
    "NOTE: For pre processing\n",
    "1. lower\n",
    "2. remove punctuation\n",
    "3. remove stopwords\n",
    "4. Add <s> and </s> for every sentence.\n",
    "'''\n",
    "\n",
    "'''\n",
    "TODO: Jinesh\n",
    "1. Parsing the custom dataset\n",
    "2. Pre processing function - Completed.\n",
    "\n",
    "parse_data -> preprocessing ->\n",
    "\n",
    "TODO: Shaan\n",
    "1. Get the infrastructure for the model set up to accept data from preprocessing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:25.438770Z",
     "start_time": "2020-10-24T04:39:24.888507Z"
    },
    "id": "x2IJbX_Mt8Gu"
   },
   "outputs": [],
   "source": [
    "# code to train your word embeddings\n",
    "\n",
    "# Read the file and prepare the training data \n",
    "# so that it is in the following format\n",
    "\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "def parse_data(training_file_path: str, percentage: int, select_column:int) -> List[str]:\n",
    "  \"\"\"This function is used to parse input lines\n",
    "  and returns a the provided percent of data.\n",
    "\n",
    "  Args:\n",
    "      lines (List[str]): list of lines\n",
    "      percentage (int): percent of the dataset needed\n",
    "      select_column (int): column to be selected from the dataset\n",
    "  Returns:\n",
    "      List[str]: lines (percentage of dataset)\n",
    "  \"\"\"\n",
    "  sentences = []\n",
    "  percentage_sentences = []\n",
    "  with open(training_file_path, \"r\", encoding=\"utf8\", errors=\"ignore\") as csvfile:\n",
    "    csv_reader = reader(csvfile)\n",
    "\n",
    "    # csv_reader_copy = csv_reader.copy()\n",
    "\n",
    "    #skipping header\n",
    "    header = next(csv_reader)\n",
    "\n",
    "    # line_length = len(list(csv_reader_copy))\n",
    "   \n",
    "    if header != None:\n",
    "      for row in csv_reader:\n",
    "        sentences.append(row[select_column])\n",
    "\n",
    "    end_of_data = int(len(sentences) * percentage * .01)\n",
    "    percentage_sentences = sentences[0:end_of_data]\n",
    "\n",
    "  return percentage_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"I am sam. Sam I am. What time is it?\" \n",
    "# -> \n",
    "# List of sentences [<s> I am sam </s>, <s> Sam I am </s> ...]\n",
    "\n",
    "def preprocessing(running_lines: List[str]) -> List[List[str]]:\n",
    "  \"\"\"This function takes in the running test and return back the\n",
    "  preprocessed text. Four tasks are done as part of this:\n",
    "    1. lower word case\n",
    "    2. remove stopwords\n",
    "    3. remove punctuation\n",
    "    4. Add <s> and </s> for every sentence\n",
    "\n",
    "  Args:\n",
    "      running_lines (List[str]): list of lines\n",
    "\n",
    "  Returns:\n",
    "      List[List[str]]: list of sentences where each sentence is broken\n",
    "                        into list of words.\n",
    "  \"\"\"\n",
    "  preprocessed_lines = []\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "  for line in running_lines:\n",
    "    lower_case_data = line.lower()\n",
    "    data_without_stop_word = remove_stopwords(lower_case_data)\n",
    "    data_without_punct = strip_punctuation(data_without_stop_word)\n",
    "    processed_data = tokenizer.tokenize(data_without_punct)\n",
    "    processed_data.insert(0,\"<s>\")\n",
    "    processed_data.append(\"</s>\")\n",
    "    preprocessed_lines.append(processed_data)\n",
    "  return preprocessed_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Provided Dataset: Spooky Dataset\n",
      "\n",
      "['<s>', 'process', 'however', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', 'circuit', 'return', 'point', 'set', 'out', 'aware', 'fact', 'perfectly', 'uniform', 'wall', '</s>']\n",
      "['<s>', 'occurred', 'fumbling', 'mere', 'mistake', '</s>']\n",
      "['<s>', 'left', 'hand', 'gold', 'snuff', 'box', 'which', 'capered', 'hill', 'cutting', 'manner', 'fantastic', 'steps', 'took', 'snuff', 'incessantly', 'air', 'greatest', 'possible', 'self', 'satisfaction', '</s>']\n",
      "['<s>', 'lovely', 'spring', 'looked', 'windsor', 'terrace', 'sixteen', 'fertile', 'counties', 'spread', 'beneath', 'speckled', 'happy', 'cottages', 'wealthier', 'towns', 'looked', 'years', 'heart', 'cheering', 'fair', '</s>']\n",
      "['<s>', 'finding', 'else', 'gold', 'superintendent', 'abandoned', 'attempts', 'perplexed', 'look', 'occasionally', 'steals', 'countenance', 'sits', 'thinking', 'desk', '</s>']\n",
      "['<s>', 'youth', 'passed', 'solitude', 'best', 'years', 'spent', 'gentle', 'feminine', 'fosterage', 'refined', 'groundwork', 'character', 'overcome', 'intense', 'distaste', 'usual', 'brutality', 'exercised', 'board', 'ship', 'believed', 'necessary', 'heard', 'mariner', 'equally', 'noted', 'kindliness', 'heart', 'respect', 'obedience', 'paid', 'crew', 'felt', 'peculiarly', 'fortunate', 'able', 'secure', 'services', '</s>']\n",
      "['<s>', 'astronomer', 'perhaps', 'point', 'took', 'refuge', 'suggestion', 'non', 'luminosity', 'analogy', 'suddenly', 'let', 'fall', '</s>']\n",
      "['<s>', 'surcingle', 'hung', 'ribands', 'body', '</s>']\n",
      "['<s>', 'knew', 'stereotomy', 'brought', 'think', 'atomies', 'theories', 'epicurus', 'since', 'discussed', 'subject', 'long', 'ago', 'mentioned', 'singularly', 'little', 'notice', 'vague', 'guesses', 'noble', 'greek', 'met', 'confirmation', 'late', 'nebular', 'cosmogony', 'felt', 'avoid', 'casting', 'eyes', 'upward', 'great', 'nebula', 'orion', 'certainly', 'expected', 'so', '</s>']\n",
      "\n",
      "Tokenizing Custom Dataset: Corona Tweets\n",
      "\n",
      "['<s>', 'menyrbie', 'phil', 'gahan', 'chrisitv', 'https', 't', 'co', 'ifz9fan2pa', 'https', 't', 'co', 'xx6ghgfzcc', 'https', 't', 'co', 'i2nlzdxno8', '</s>']\n",
      "['<s>', 'advice', 'talk', 'neighbours', 'family', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'phone', 'numbers', 'neighbours', 'schools', 'employer', 'chemist', 'gp', 'set', 'online', 'shopping', 'accounts', 'poss', 'adequate', 'supplies', 'regular', 'meds', 'order', '</s>']\n",
      "['<s>', 'coronavirus', 'australia', 'woolworths', 'elderly', 'disabled', 'dedicated', 'shopping', 'hours', 'amid', 'covid', '19', 'outbreak', 'https', 't', 'co', 'binca9vp8p', '</s>']\n",
      "['<s>', 'food', 'stock', 'empty', 'please', 'don', 't', 'panic', 'food', 'need', 'stay', 'calm', 'stay', 'safe', 'covid19france', 'covid', '19', 'covid19', 'coronavirus', 'confinement', 'confinementotal', 'confinementgeneral', 'https', 't', 'co', 'zrlg0z520j', '</s>']\n",
      "['<s>', 'me', 'ready', 'supermarket', 'covid19', 'outbreak', 'i', 'm', 'paranoid', 'food', 'stock', 'litteraly', 'empty', 'coronavirus', 'thing', 'please', 'don', 't', 'panic', 'causes', 'shortage', 'coronavirusfrance', 'restezchezvous', 'stayathome', 'confinement', 'https', 't', 'co', 'usmualq72n', '</s>']\n",
      "['<s>', 'news', 'region', 's', 'confirmed', 'covid', '19', 'case', 'came', 'sullivan', 'county', 'week', 'people', 'flocked', 'area', 'stores', 'purchase', 'cleaning', 'supplies', 'hand', 'sanitizer', 'food', 'toilet', 'paper', 'goods', 'tim', 'dodson', 'reports', 'https', 't', 'co', 'cfxch7a2lu', '</s>']\n",
      "['<s>', 'cashier', 'grocery', 'store', 'sharing', 'insights', 'covid', '19', 'prove', 'credibility', 'commented', 'i', 'm', 'civics', 'class', 'know', 'i', 'm', 'talking', 'about', 'https', 't', 'co', 'iefdnehgdo', '</s>']\n",
      "['<s>', 'supermarket', 'today', 'didn', 't', 'buy', 'toilet', 'paper', 'rebel', 'toiletpapercrisis', 'covid', '19', 'https', 't', 'co', 'evxkqlidaz', '</s>']\n",
      "['<s>', 'covid', '19', 'retail', 'store', 'classroom', 'atlanta', 'open', 'walk', 'in', 'business', 'classes', 'weeks', 'beginning', 'monday', 'march', '16', 'continue', 'process', 'online', 'phone', 'orders', 'normal', 'thank', 'understanding', 'https', 't', 'co', 'kw91zj5o5i', '</s>']\n",
      "['<s>', 'corona', 'prevention', 'we', 'stop', 'buy', 'things', 'cash', 'use', 'online', 'payment', 'methods', 'corona', 'spread', 'notes', 'prefer', 'online', 'shopping', 'home', 'it', 's', 'time', 'fight', 'covid', '19', 'govindia', 'indiafightscorona', '</s>']\n",
      "['<s>', 'month', 'hasn', 't', 'crowding', 'supermarkets', 'restaurants', 'reducing', 'hours', 'closing', 'malls', 'means', 'entrance', 'dependent', 'single', 'supermarket', 'manila', 'lockdown', 'covid2019', 'philippines', 'https', 't', 'co', 'hxws9lanf9', '</s>']\n",
      "['<s>', 'covid', '19', 'situation', 'increased', 'demand', 'food', 'products', 'wait', 'time', 'longer', 'online', 'orders', 'particularly', 'beef', 'share', 'freezer', 'packs', 'thank', 'patience', 'time', '</s>']\n",
      "['<s>', 'horningsea', 'caring', 'community', 'let', 's', 'look', 'capable', 'village', 'ensure', 'stay', 'healthy', 'bringing', 'shopping', 'doors', 'help', 'online', 'shopping', 'self', 'isolation', 'symptoms', 'exposed', 'somebody', 'has', 'https', 't', 'co', 'lsgrxxhjhh', '</s>']\n",
      "['<s>', 'me', 'don', 't', 'need', 'stock', 'food', 'i', 'll', 'amazon', 'deliver', 'need', 'coronavirus', 'amazon', 'https', 't', 'co', '8ywakfjexc', '</s>']\n",
      "['<s>', 'adara', 'releases', 'covid', '19', 'resource', 'center', 'travel', 'brands', 'insights', 'help', 'travel', 'brands', 'stay', 'up', 'to', 'date', 'consumer', 'travel', 'behavior', 'trends', 'https', 't', 'co', 'pna797jdkv', 'https', 't', 'co', 'dqox6usihz', '</s>']\n",
      "['<s>', 'lines', 'grocery', 'store', 'unpredictable', 'eating', 'safe', 'alternative', 'avoiding', 'restaurants', 'right', 'now', 'https', 't', 'co', '9idzsis5oq', 'coronavirus', 'covid19', 'https', 't', 'co', 'zhbh898lf6', '</s>']\n",
      "['<s>', '13', 'https', 't', 'co', '51bl8p6vzh', '</s>']\n",
      "['<s>', 'eyeonthearctic', '16mar20', 'russia', 'consumer', 'surveillance', 'watchdog', 'reported', 'case', 'high', 'arctic', 'man', 'traveled', 'iran', 'covid', '19', '101', 'observed', 'https', 't', 'co', '4wnrrk9okc', 'https', 't', 'co', 'ld05k5eyns', '</s>']\n",
      "['<s>', 'amazon', 'glitch', 'stymies', 'foods', 'fresh', 'grocery', 'deliveries', 'as', 'covid', '19', 'spread', 'we', 've', 'seen', 'significant', 'increase', 'people', 'shopping', 'online', 'groceries', 'spokeswoman', 'said', 'statement', 'today', 'resulted', 'systems', 'impact', 'affecting', 'https', 't', 'co', 'tbzz2mc3b3', '</s>']\n",
      "['<s>', 'aren', 't', 'struggling', 'consider', 'donating', 'food', 'bank', 'nonprofit', 'demand', 'services', 'increase', 'covid', '19', 'impacts', 'jobs', 'people', 's', 'way', 'life', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# parse and preprocess provided data\n",
    "print(\"Tokenizing Provided Dataset: Spooky Dataset\\n\")\n",
    "spooky_sentences = preprocessing(parse_data(\"data/provided/train/train.csv\", .05, 1))\n",
    "for token in spooky_sentences:\n",
    "  print(token)\n",
    "# parse and preprocess custom data\n",
    "print(\"\\nTokenizing Custom Dataset: Corona Tweets\\n\")\n",
    "covid_sentences = preprocessing(parse_data(\"data/custom/train/Corona_NLP_train.csv\", .05, 4))\n",
    "for token in covid_sentences:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-1.4 Choosing the dataset:\n",
    "Coronavirus tweets NLP\n",
    "https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    },
    "id": "Od_L53GEt8Gv"
   },
   "outputs": [],
   "source": [
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "sg = 1 # The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "window = 5\n",
    "size = EMBEDDINGS_SIZE\n",
    "min_count = 1\n",
    "workers = multiprocessing.cpu_count()\n",
    "sorted_vocab = 1 #1 for descending order\n",
    "\n",
    "def generate_embeddings(model_name: str, sentences: List[List[str]]) -> (str, str):\n",
    "\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences, \n",
    "        vector_size=size, \n",
    "        window=window, \n",
    "        min_count=min_count, \n",
    "        workers=workers, \n",
    "        sg=sg,\n",
    "        sorted_vocab=sorted_vocab)\n",
    "    model_path = \"word2vec.\" + model_name + \".model\"\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Store just the words + their trained embeddings.\n",
    "    word_vectors_path = \"word2vec.\" + model_name + \".wordvectors\"\n",
    "    word_vectors = model.wv\n",
    "    word_vectors.save(word_vectors_path)\n",
    "\n",
    "    return model_path, word_vectors_path\n",
    "\n",
    "spooky_model_path, spooky_word_vectors_path = generate_embeddings(\"spooky\", spooky_sentences)\n",
    "\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "# wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
    "# vector = wv['computer']  # Get numpy vector of a word\n",
    "# print(vector);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:43.448249Z",
     "start_time": "2020-10-24T04:39:43.444835Z"
    },
    "id": "xrt52ahnt8Gw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 159\n"
     ]
    }
   ],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "model = Word2Vec.load(spooky_model_path)\n",
    "print('Vocab size {}'.format(len(model.wv)))\n",
    "\n",
    "# print('Word counts {}'.format(model.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    },
    "id": "UUanXgQLt8Gy"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "model.wv.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGMUTMcmt8G0"
   },
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "sN0KvmUKt8G0"
   },
   "outputs": [],
   "source": [
    "# then do a second data set\n",
    "covid_model_path, covid_word_vectors_path = generate_embeddings(\"covid\", covid_sentences)\n",
    "\n",
    "# model = Word2Vec.load(covid_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsjzTVFjt8G1"
   },
   "source": [
    "What text-normalization and pre-processing did you do and why? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOFmHpH8t8G2"
   },
   "source": [
    "Step 2: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shaan:\n",
    "- Dump the images for both models with varying perplexity values\n",
    "- Figure out which images we're going to do analysis on 3-10\n",
    "- Figure out which features we're going to write about\n",
    "\n",
    "Jinesh:\n",
    "- Separating functionality into different cells\n",
    "- Making sure the code still works\n",
    "\n",
    "For next time:\n",
    "- Analysis on the images\n",
    "- Training feed forward neural network\n",
    "- Generation of sentences and comparing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "NXjy2-OqgvIf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('point', 2), ('gold', 2), ('snuff', 2), ('took', 2), ('looked', 2), ('years', 2), ('heart', 2), ('felt', 2), ('process', 1), ('however', 1)]\n",
      "['point', 'gold', 'snuff', 'took', 'looked', 'years', 'heart', 'felt', 'process', 'however']\n"
     ]
    }
   ],
   "source": [
    "wv_spooky = KeyedVectors.load(spooky_word_vectors_path, mmap='r')\n",
    "wv_covid = KeyedVectors.load(covid_word_vectors_path, mmap='r')\n",
    "\n",
    "def generate_vocab(sentences):\n",
    "\n",
    "    word_counts = Counter()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for i in sentence:\n",
    "            word_counts[i] += 1\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "def get_top_k_words(sentences, k):\n",
    "\n",
    "    word_counts = generate_vocab(sentences)\n",
    "\n",
    "    word_counts[\"<s>\"] = 0\n",
    "    word_counts[\"</s>\"] = 0\n",
    "\n",
    "    k_most_common = []\n",
    "\n",
    "    for i in word_counts.most_common(k):\n",
    "        k_most_common.append(i[0])\n",
    "\n",
    "    print(word_counts.most_common(k)) #Seeing the corresponding counts\n",
    "    return k_most_common\n",
    "\n",
    "keys  = get_top_k_words(spooky_sentences, 10)\n",
    "print(keys) \n",
    "\n",
    "# print(model.wv.most_similar(keys[0], topn=10))\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    # print(model.wv.most_similar(word, topn=10))\n",
    "    for similar_word, _ in model.wv.most_similar(word, topn=10):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "\n",
    "# for i in word_clusters:\n",
    "#     print(i)\n",
    "\n",
    "# for i in embedding_clusters:\n",
    "#     print(i)\n",
    "\n",
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, perplexity, filename=None):\n",
    "\n",
    "    embedding_clusters = np.array(embedding_clusters)\n",
    "    n, m, k = embedding_clusters.shape\n",
    "    tsne_model_en_2d = TSNE(perplexity=perplexity, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "    embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embeddings_en_2d, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# for i in range(5, 96, 5):\n",
    "#     file_name = 'spooky_similar_words_perplexity_' + str(i) + '.png'\n",
    "#     tsne_plot_similar_words('Similar words from Spooky Dataset', keys, embedding_clusters, word_clusters, 0.7, i,\n",
    "#     file_name)\n",
    "\n",
    "# perplexity = 101\n",
    "# file_name = 'similar_words_perplexity_' + str(perplexity) + '.png'\n",
    "# tsne_plot_similar_words('Similar words from Covid Dataset', keys, embedding_clusters, word_clusters, 0.7, perplexity,\n",
    "#     file_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWlWydbrgv4P"
   },
   "source": [
    "##Write down your analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tmrTVDqt8G2"
   },
   "source": [
    "Cite your sources:\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix2On6zat8G2"
   },
   "source": [
    "Step 3: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZsCKQWDt8G2"
   },
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    },
    "id": "ec0KKYj0t8G3"
   },
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# The size of the ngram language model you want to train\n",
    "# change as needed for your experiments\n",
    "NGRAM = 3 \n",
    "\n",
    "# Initializing a Tokenizer\n",
    "\n",
    "spooky_vocab = list(generate_vocab(spooky_sentences).keys())\n",
    "# covid_vocab = generate_vocab(covid_sentences).keys()\n",
    "\n",
    "dict_words = {word:n for n, word in enumerate(spooky_vocab)}\n",
    "\n",
    "\n",
    "def texts_to_sequences(tokenized_sentences, dict_words):\n",
    "\n",
    "    encoded = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        encoded_sentence = []\n",
    "        for token in sentence:\n",
    "            encoded_sentence.append(dict_words[token])\n",
    "        encoded.append(encoded_sentence)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "encoded_sentences = texts_to_sequences(spooky_sentences, dict_words)\n",
    "\n",
    "# for i in encoded_sentences:\n",
    "#     print(i)\n",
    "            \n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCndArPmt8G5"
   },
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jG42_9Xt8G6"
   },
   "source": [
    "#### Fixed ngram based sequences "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HsoPVS8ct8G7"
   },
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process               however\n",
    "    process, however               afforded\n",
    "    however, afforded\t           me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    },
    "id": "B_4YqhKTt8G7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2)\n",
      "(1, 2, 3)\n",
      "(2, 3, 4)\n",
      "(3, 4, 5)\n",
      "(4, 5, 6)\n",
      "(5, 6, 7)\n",
      "(6, 7, 8)\n",
      "(7, 8, 9)\n",
      "(8, 9, 10)\n",
      "(9, 10, 11)\n",
      "(10, 11, 12)\n",
      "(11, 12, 13)\n",
      "(12, 13, 14)\n",
      "(13, 14, 15)\n",
      "(14, 15, 16)\n",
      "(15, 16, 17)\n",
      "(16, 17, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<zip at 0x13da1d100>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngram_training_samples(ngram: list) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    \n",
    "    generated_ngrams = ngrams(ngram[0], NGRAM)\n",
    "\n",
    "    for gram in generated_ngrams:\n",
    "        print(gram)\n",
    "\n",
    "    return generated_ngrams\n",
    "\n",
    "    \n",
    "generate_ngram_training_samples(encoded_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWL6Czlxt8G8"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:31.213422Z",
     "start_time": "2020-10-24T05:21:31.061759Z"
    },
    "id": "csweN-d1t8G9"
   },
   "outputs": [],
   "source": [
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    },
    "id": "Vjr6vwP5t8G9"
   },
   "outputs": [],
   "source": [
    "def read_embeddings():\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    '''\n",
    "    \n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':[0....], ...}\n",
    "    # index to embedding : {1:[0....], ...} \n",
    "    # use your tokenizer's word_index to find the index of\n",
    "    # a given word\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    },
    "id": "H6g9g7p6t8G9"
   },
   "outputs": [],
   "source": [
    "def data_generator(X: list, y: list, num_sequences_per_batch: int) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:55.470133Z",
     "start_time": "2020-10-24T05:22:55.398259Z"
    },
    "id": "vgXSWdlMt8G-"
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "# initialize data_generator\n",
    "# num_sequences_per_batch = 128 # this is the batch size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzfweqz1t8G-"
   },
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:56:19.207252Z",
     "start_time": "2020-10-24T04:56:19.204894Z"
    },
    "id": "4fZlHukVt8G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    },
    "id": "KmgNnQj5t8G_"
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "\n",
    "# Define the model architecture using Keras Sequential API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "id": "-dWf2qO3t8G_"
   },
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "model.fit(x=train_generator, \n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVjtknkVt8HA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCZ2S5mpt8HA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QwRhKYwt8HA"
   },
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    },
    "id": "ewR5ueOJt8HB"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list, \n",
    "                 n_words: int):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:14:13.123529Z",
     "start_time": "2020-10-24T04:14:13.000264Z"
    },
    "id": "XZ9fShSyt8HB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w68JVS2jt8HB"
   },
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE4dcQdut8HC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yet5p8N1t8HC"
   },
   "source": [
    "Sources Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sPu_1h2t8HC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wordembeddings_starter.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
