{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjJn2R-Ft8Gi"
   },
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: __Shaan Hossain, Jinesh Shailesh Mehta__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwlPkjgt8Gq"
   },
   "source": [
    "Step 1: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "(describe the provided dataset that you have chosen here)\n",
    "\n",
    "__From the given two choices, we have [Spooky Authors Dataset](https://www.kaggle.com/c/spooky-author-identification).__\n",
    "\n",
    "Describe what data set you have chosen to compare and contrast with the your chosen provided dataset. Make sure to describe where it comes from and it's general properties.\n",
    "\n",
    "NOTE: We'll refer to the Spooky Authors Dataset as the Spooky Dataset.\n",
    "\n",
    "Description of our dataset which we'll refer to as the Covid Dataset.\n",
    "\n",
    "- Our selected data: [Coronavirus tweets NLP](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification)\n",
    "- Description : The tweets have been pulled from Twitter and manual tagging has been done then. The names and usernames have been given codes to avoid any privacy concerns.\n",
    "- Columns: \n",
    "    - UserName\n",
    "    - ScreenName\n",
    "    - Location\n",
    "    - TweetAt\n",
    "    - OriginalTweet\n",
    "    - Sentiment\n",
    "\n",
    "<br>\n",
    "\n",
    "Similarities with chosen provided dataset:\n",
    "- There are long tweets in the Covid dataset that resemble the longer more coherent sentences in the Spooky dataset.\n",
    "- Both datasets are in English and roughly follow the English rules of grammar.\n",
    "- Every entry is associated with an author in both datasets whether it's a person tweeting or an author. You can also identify each entry with a unique ID.\n",
    "\n",
    "<br>\n",
    "\n",
    "Constrasts with chosen provided dataset:\n",
    "\n",
    "- There are a variety of contrasts between the datasets, but one of the most striking differences is the increased vocabulary size in the covid dataset versus the spooky datasets by a factor of 3 at 100% of each dataset. One reason this might happen is due to the medium of the Covid dataset of Twitter. There are quite a few random strings of jibberish that often occur only once in the Tweets when someone randomly smashes their keyboard.\n",
    "- We noticed that the Covid dataset has a lot of short sentences. We're not sure how this impacts training, but it's important to note. If you have larger more complex sentences, it's possible that there is more context for the ngrams versus the short tweets in the Covid dataset. \n",
    "- The sentences in the Spooky dataset are much more coherent and tend to have better grammar than the Covid dataset. With respect to this task, having formalized grammar, proper spelling, and more consistency could definitely impact the model during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    },
    "id": "uQLg8dGdt8Gr"
   },
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "from typing import List, Dict\n",
    "# libs used for preprocessing\n",
    "from gensim.parsing.preprocessing import stem_text, remove_stopwords, strip_punctuation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import ngrams\n",
    "# libs used for file reading and parsing\n",
    "from csv import reader\n",
    "# libs used for plotting\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "# libs used for generating word2vec models\n",
    "from gensim.models import Word2Vec\n",
    "# to compute the training using multiple threads\n",
    "from multiprocessing import cpu_count\n",
    "# other utilities\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from os import path\n",
    "import random\n",
    "# libs used for validating the word vectors and loading word vectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines which dataset to create embeddings and\n",
    "# train the neural network on\n",
    "dataset_to_use = 'Covid' # either 'Spooky' or 'Covid'\n",
    "dataset_percentage = 30 # percentage range 1 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables initialization\n",
    "training_file = \"\"\n",
    "column_to_parse = None\n",
    "if dataset_to_use == 'Spooky':\n",
    "    training_file = \"data/provided/train/train.csv\"\n",
    "    column_to_parse = 1\n",
    "else:\n",
    "    training_file = \"data/custom/train/Corona_NLP_train.csv\"\n",
    "    column_to_parse = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj0A0mCkt8Gt"
   },
   "source": [
    "### a) Train embeddings on GIVEN dataset : Spooky Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Function to parse data from training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:25.438770Z",
     "start_time": "2020-10-24T04:39:24.888507Z"
    },
    "id": "x2IJbX_Mt8Gu"
   },
   "outputs": [],
   "source": [
    "# Read the file and prepare the training data \n",
    "# so that it is in the following format\n",
    "\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "def parse_data(training_file_path: str, percentage: int, select_column:int) -> List[str]:\n",
    "  \"\"\"This function is used to parse input lines\n",
    "  and returns a the provided percent of data.\n",
    "\n",
    "  Args:\n",
    "      lines (List[str]): list of lines\n",
    "      percentage (int): percent of the dataset needed\n",
    "      select_column (int): column to be selected from the dataset\n",
    "  Returns:\n",
    "      List[str]: lines (percentage of dataset)\n",
    "  \"\"\"\n",
    "  sentences = []\n",
    "  percentage_sentences = []\n",
    "  with open(training_file_path, \"r\", encoding=\"utf8\", errors=\"ignore\") as csvfile:\n",
    "    csv_reader = reader(csvfile)\n",
    "    #skipping header\n",
    "    header = next(csv_reader)\n",
    "\n",
    "    # line_length = len(list(csv_reader_copy))\n",
    "   \n",
    "    if header != None:\n",
    "      for row in csv_reader:\n",
    "        sentences.append(row[select_column])\n",
    "\n",
    "    end_of_data = int(len(sentences) * percentage * .01)\n",
    "    percentage_sentences = sentences[0:end_of_data]\n",
    "\n",
    "  return percentage_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Function to do preprocessing for the parsed data.\n",
    "    - Lower word case\n",
    "    - Remove stopwords\n",
    "    - Remove punctuations\n",
    "    - Tokenize line and add sentence seperator tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(running_lines: List[str]) -> List[List[str]]:\n",
    "  \"\"\"This function takes in the running test and return back the\n",
    "  preprocessed text. Four tasks are done as part of this:\n",
    "    1. lower word case\n",
    "    2. remove stopwords\n",
    "    3. remove punctuation\n",
    "    4. Add - <s> and </s> for every sentence\n",
    "\n",
    "  Args:\n",
    "      running_lines (List[str]): list of lines\n",
    "\n",
    "  Returns:\n",
    "      List[List[str]]: list of sentences where each sentence is broken\n",
    "                        into list of words.\n",
    "  \"\"\"\n",
    "  preprocessed_lines = []\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "  for line in running_lines:\n",
    "    lower_case_data = line.lower()\n",
    "    data_without_stop_word = remove_stopwords(lower_case_data)\n",
    "    data_without_punct = strip_punctuation(data_without_stop_word)\n",
    "    processed_data = tokenizer.tokenize(data_without_punct)\n",
    "    processed_data.insert(0,\"<s>\")\n",
    "    processed_data.append(\"</s>\")\n",
    "    preprocessed_lines.append(processed_data)\n",
    "  return preprocessed_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Computing the parsing and preprocessing for both the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse and preprocess data\n",
    "print(f'Tokenizing Provided Dataset: {dataset_to_use} Dataset\\n')\n",
    "sentences = preprocessing(parse_data(training_file, dataset_percentage, column_to_parse))\n",
    "# for token in sentences:\n",
    "#   print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Provide functionality for generating word embeddings for a given model.\n",
    "\n",
    "Note: For the Word2Vec model training, we are using Skip Gram model as shown with value sg in the below code. Also, we are keep generating the vocab in sorted order (descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    },
    "id": "Od_L53GEt8Gv"
   },
   "outputs": [],
   "source": [
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "sg = 1 # The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "window = 5\n",
    "size = EMBEDDINGS_SIZE\n",
    "min_count = 1\n",
    "workers = cpu_count()\n",
    "sorted_vocab = 1 #1 for descending order\n",
    "\n",
    "def generate_embeddings(model_name: str, sentences: List[List[str]]) -> (str, str):\n",
    "    \"\"\"This function is used to generate embeddings (model and word vectors) for a given\n",
    "    model name and provided sentences.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): name of the model\n",
    "        sentences (List[List[str]]): sentences to be used for model creation\n",
    "\n",
    "    Returns:\n",
    "        str, str : model path and word vector path\n",
    "    \"\"\"\n",
    "    #generate word2vec model\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences, \n",
    "        vector_size=size, \n",
    "        window=window, \n",
    "        min_count=min_count, \n",
    "        workers=workers, \n",
    "        sg=sg,\n",
    "        sorted_vocab=sorted_vocab)\n",
    "    # save model\n",
    "    model_file_name = f'word2vec.{model_name}.model'\n",
    "    model_path = f'{model_name}/model'\n",
    "    # make sure all the parent folders exists \n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "    model_file_path = f'{model_path}/{model_file_name}'\n",
    "    model.save(model_file_path)\n",
    "    # Store just the words + their trained embeddings.\n",
    "    word_vectors_file_name = f'word2vec.{model_name}.wordvectors'\n",
    "    word_vectors = model.wv\n",
    "    word_vectors_path = f'{model_name}/wordvectors'\n",
    "    # make sure all the parent folders exists \n",
    "    Path(word_vectors_path).mkdir(parents=True, exist_ok=True)\n",
    "    word_vectors_file_path = f'{word_vectors_path}/{word_vectors_file_name}'\n",
    "    word_vectors.save(word_vectors_file_path)\n",
    "    return model_file_path, word_vectors_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Generate model and word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path, word_vectors_path = generate_embeddings(dataset_to_use, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Display the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:43.448249Z",
     "start_time": "2020-10-24T04:39:43.444835Z"
    },
    "id": "xrt52ahnt8Gw"
   },
   "outputs": [],
   "source": [
    "# print out the vocabulary size for defined dataset\n",
    "word_to_vec_model = Word2Vec.load(model_path)\n",
    "print('Vocab size: {} for {} Dataset.'.format(len(word_to_vec_model.wv), dataset_to_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Saving the word embeddings in a text file (load later if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    },
    "id": "UUanXgQLt8Gy"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "# make sure all the parent folders exists \n",
    "embeddings_folder_path = f'{dataset_to_use}/embeddings'\n",
    "Path(embeddings_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "word_to_vec_model.wv.save_word2vec_format(f'{embeddings_folder_path}/{dataset_to_use}_embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGMUTMcmt8G0"
   },
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN0KvmUKt8G0"
   },
   "outputs": [],
   "source": [
    "# For getting word embeddings for covid dataset, just make 'dataset_to_use' defined in second code cell to \n",
    "# \"Covid\" and it will generate the respective embeddings in the 'model_name/embeddings' folder with the \n",
    "# name 'model_name/embeddings.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsjzTVFjt8G1"
   },
   "source": [
    "What text-normalization and pre-processing did you do and why?\n",
    "\n",
    "- Text Normalization and Pre-processing steps done:\n",
    "    - Lower word case\n",
    "    - Remove stopwords\n",
    "    - Remove punctuations\n",
    "    - Tokenize line and add sentence seperator tokens\n",
    "\n",
    "- Lower word case:\n",
    "\n",
    "    - This normalization step reduces our vocabulary size and allows fewer embeddings, which decreased runtime and increased performance relative to accuracy.\n",
    "\n",
    "- Remove stopwords:\n",
    "\n",
    "    - Removing the stop words will reduce the size of the vocabulary, and these high frequency words can skew the predictions of the model.\n",
    "\n",
    "    - Our vocabulary is again reduced, which will decrease training times. \n",
    "\n",
    "- Remove punctations:\n",
    "\n",
    "    - Same reason as lower word case. \n",
    "\n",
    "- Tokenizing line and separatign by sentence tokens:\n",
    "\n",
    "    - This adds extra context onto the words and practically speaking had a drastic effect on accuracy. For predicting the next word, this extra context seemed to be useful. On the Spooky dataset it increased the accuracy from roughly .05% without it to roughly 9% with the sentence tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOFmHpH8t8G2"
   },
   "source": [
    "Step 2: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce any graphs or figures that illustrate what you have found and write 2 - 3 paragraphs describing the differences you find between your two sets of embeddings and why you see them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Defining functions to generate vocab and get top k frequent words from the parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(sentences:List[List[str]])->Dict:\n",
    "    \"\"\"This function is used to generate vocabs from the given sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): sentences\n",
    "\n",
    "    Returns:\n",
    "        dict: word with each having count\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        for i in sentence:\n",
    "            word_counts[i] += 1\n",
    "    return word_counts\n",
    "\n",
    "def get_top_k_words(sentences:str, k:int)->Dict:\n",
    "    \"\"\"This function is used to get top k word from the given\n",
    "    sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (str): list of all the sentences to be used for count\n",
    "        k (int): top k words (max frequency)\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: k words with their count\n",
    "    \"\"\"\n",
    "    word_counts = generate_vocab(sentences)\n",
    "    word_counts[\"<s>\"] = 0\n",
    "    word_counts[\"</s>\"] = 0\n",
    "    k_most_common = []\n",
    "    for i in word_counts.most_common(k):\n",
    "        k_most_common.append(i[0])\n",
    "    # print(word_counts.most_common(k)) #Seeing the corresponding counts\n",
    "    return k_most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXjy2-OqgvIf"
   },
   "outputs": [],
   "source": [
    "TOP_N_KEYS = 10\n",
    "wv = KeyedVectors.load(word_vectors_path, mmap='r')\n",
    "keys = get_top_k_words(sentences, TOP_N_KEYS)\n",
    "# print(keys) \n",
    "# print(model.wv.most_similar(keys[0], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define embedding clusters and word clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_word_clusters(model, keys:Dict):\n",
    "    '''\n",
    "    This function is used to generate embedding and word clusters\n",
    "    for the given model and the respective keys.\n",
    "\n",
    "    Args:\n",
    "        model (word2vec): model to be used\n",
    "        keys (Dict): keys which are most frequent in the model\n",
    "\n",
    "    Returns:\n",
    "        List, List: embedding and word cluster\n",
    "    '''\n",
    "    embedding_clusters = []\n",
    "    word_clusters = []\n",
    "    for word in keys:\n",
    "        embeddings = []\n",
    "        words = []\n",
    "        # print(model.wv.most_similar(word, topn=10))\n",
    "        for similar_word, _ in model.wv.most_similar(word, topn=10):\n",
    "            words.append(similar_word)\n",
    "            embeddings.append(model.wv[similar_word])\n",
    "        embedding_clusters.append(embeddings)\n",
    "        word_clusters.append(words)\n",
    "    return embedding_clusters, word_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define t-sne projection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot_similar_words(title:str, labels:List, embedding_clusters:List, word_clusters:List,\n",
    "                            a:float, perplexity:int, filename:str=None):\n",
    "    \"\"\"This function is used to generate tsne plot projections for given parameters.\n",
    "\n",
    "    Args:\n",
    "        title (str): plot title\n",
    "        labels (List): list of labels\n",
    "        embedding_clusters (List): list of embedding clusters\n",
    "        word_clusters (List): list of word clusters\n",
    "        a (float): alpha value for plot\n",
    "        perplexity (int): perplexity value\n",
    "        filename (str, optional): file name for the plot saving. Defaults to None.\n",
    "    \"\"\"\n",
    "    embedding_clusters = np.array(embedding_clusters)\n",
    "    n, m, k = embedding_clusters.shape\n",
    "    tsne_model_en_2d = TSNE(perplexity=perplexity, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "    embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embeddings_en_2d, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Generate embedding and word clusters. Also, generate 2d projections for spooky and covid dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERPLEXITY = 100\n",
    "ALPHA_VALUE = 0.7\n",
    "embeddings_clusters, word_clusters = generate_embeddings_word_clusters(model, keys)\n",
    "# for i in range(1, 30, 3):\n",
    "#     file_name = 'similar_words_perplexity_' + str(i) + '.png'\n",
    "#     tsne_plot_similar_words('Similar words', keys, embedding_clusters, word_clusters, 0.7, i,\n",
    "#     file_name)\n",
    "images_folder_path = f'{dataset_to_use}/images'\n",
    "Path(images_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "plot_file_path = f'{images_folder_path}/similar_words_perplexity_{PERPLEXITY}.png'\n",
    "tsne_plot_similar_words(f'Similar words from {dataset_to_use} Dataset', keys, embeddings_clusters, word_clusters, ALPHA_VALUE, PERPLEXITY,\n",
    "    plot_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWlWydbrgv4P"
   },
   "source": [
    "#### 7. Produce any graphs or figures that illustrate what you have found and write 2 - 3 paragraphs describing the differences you find between your two sets of embeddings and why you see them.\n",
    "\n",
    "NOTE: The graphs we're analyzing are called covid_perplexity_100.png and spooky_perplexity_100.png inside the analysis folder. Running our notebook will generate new images with perplexity 100 in the output cell as well as the images folder under each dataset.\n",
    "\n",
    "1. Introductory for generation of graphs:\n",
    "    - For the generation of the projection, we consider the top-k-words which are going to be most similar. For this homework, we assume the value to be 10. Next, we generated the embedding clusters and word clusters for those 10 words and created projection graphs. For the analysis part, we started looking into different range of perplexity for both the datasets starting from 0 to 10 and finally reached to the range close to 100. For the range nearer to 100, we got some converage for both the datasets, especially at the range from 95 to 103. Looking at these findings, we generate the graph at 100 perplexity and highlight some findings. \n",
    "2. Describe Spooky\n",
    "    - In this specific projection of our spooky dataset with perplexity 100, we've noticed that there are 4 groups. The top three groups appear to be tightly clustered, while the bottom group had a tight cluster to the middle center with scattered data points to the right. The scattered datapoints unravel slowly with 2 single outliers that are on top of each other. These are the points closest to the bottom right corner.\n",
    "\n",
    "3. Describe Covid\n",
    "    - There are 6 groups roughly speaking in the Covid dataset. The opposing clusters in the bottom left and top right corner appear to hold a large number of points each. The top left group is quite scattered. There are two outliers between the top and bottom left groups. There are 3 groups loosely arranged in the bottom right, which appear to be evenly distributed relative to each other. \n",
    "\n",
    "4. Compare and contract the embeddings\n",
    "    - The Covid dataset appeared to have the two densest clusters at the opposing bottom left and top right corners while the Spooky dataset has data that's more tightly clustered overall. The Covid dataset appears to have more evenly distributed data in each of the two tightly clustered groups and the other 4 groups. From these comparisons of the two datasets, Spooky embeddings appear to be more easily separated into groups that are closely related, and the Covid embeddings suggest that the words themselves are less related. This would make sense given the higher vocabulary of the Covid dataset and the broader domain of the Covid dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tmrTVDqt8G2"
   },
   "source": [
    "Cite your sources:\n",
    "- [Understanding Overview of the TSNE plot](https://www.bioinformatics.babraham.ac.uk/projects/seqmonk/Help/3%20Visualisation/3.2%20Figures%20and%20Graphs/3.2.26%20The%20TSNE%20Plot.html)\n",
    "- [Sample example with digits](https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_tsne.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix2On6zat8G2"
   },
   "source": [
    "Step 3: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZsCKQWDt8G2"
   },
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define a function that would convert text to seqeunces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(tokenized_sentences:List[List[str]], vocab_index_mapping:dict) -> List[List[int]]:\n",
    "    \"\"\"This function is used to generate sequences from the given texts.\n",
    "\n",
    "    Args:\n",
    "        tokenized_sentences (List[List[str]]): list of list of tokens (all sentences)\n",
    "        vocab_index_mapping (dict): vocab mapped to index\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: list of encoded sentences\n",
    "    \"\"\"\n",
    "    encoded_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        encoded_sentence = []\n",
    "        for token in sentence:\n",
    "            encoded_sentence.append(vocab_index_mapping[token])\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "    return encoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate and encode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    },
    "id": "ec0KKYj0t8G3"
   },
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# The size of the ngram language model you want to train\n",
    "# change as needed for your experiments\n",
    "NGRAM = 3\n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "vocab = list(generate_vocab(sentences).keys())\n",
    "# word -> index\n",
    "vocab_index_mapping = {word:index for index, word in enumerate(vocab)}\n",
    "encoded_sentences = texts_to_sequences(sentences, vocab_index_mapping)\n",
    "# for sentence in encoded_sentences:\n",
    "#     print(sentence)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCndArPmt8G5"
   },
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jG42_9Xt8G6"
   },
   "source": [
    "#### Fixed ngram based sequences "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HsoPVS8ct8G7"
   },
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process                                  however\n",
    "    process, however                                  afforded\n",
    "    however, afforded\t                              me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define function to generate ngrams from training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    },
    "id": "B_4YqhKTt8G7"
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded_sentences: list) -> list:\n",
    "    \"\"\"Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "\n",
    "    Args:\n",
    "        encoded_sentences (list): encoded sentences\n",
    "\n",
    "    Returns:\n",
    "        list: generated ngrams from encoded sentences \n",
    "        - list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    \"\"\"    \n",
    "    generated_ngrams_list = []\n",
    "    for sentence in encoded_sentences:\n",
    "        generated_ngrams_list.append(list(ngrams(sentence, NGRAM + 1)))\n",
    "\n",
    "    #enable to print generated ngrams\n",
    "    # for generated_ngrams in generated_ngrams_list:\n",
    "    #     for generated_ngram in generated_ngrams:\n",
    "    #         print(generated_ngram)\n",
    "\n",
    "    return generated_ngrams_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generating ngrams for encoded sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ngrams = generate_ngram_training_samples(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWL6Czlxt8G8"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Defining function to split ngrams into X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:31.213422Z",
     "start_time": "2020-10-24T05:21:31.061759Z"
    },
    "id": "csweN-d1t8G9"
   },
   "outputs": [],
   "source": [
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]\n",
    "\n",
    "def split_ngram_to_training_sample(generated_ngrams: list):\n",
    "    \"\"\"This function is used to split the provided n grams into X and Y.\n",
    "\n",
    "    Args:\n",
    "        generated_ngrams (list): ngrams to be splitted\n",
    "\n",
    "    Returns:\n",
    "        X, Y: List X, List Y\n",
    "    \"\"\"\n",
    "    generated_n_grams_copy = generated_ngrams.copy()\n",
    "    X = []\n",
    "    Y = []\n",
    "    for ngrams in generated_n_grams_copy:\n",
    "        for ngram in ngrams:\n",
    "            ngram = list(ngram)\n",
    "            Y.append(ngram.pop(len(ngram) - 1))\n",
    "            X.append(ngram)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Generating X and Y from generated ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = split_ngram_to_training_sample(generated_ngrams)\n",
    "# for i in range(0, len(X)):\n",
    "#     print(\"X: \" + str(X[i]))\n",
    "#     print(\"Y: \" + str(Y[i]))\n",
    "\n",
    "# print(X)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Defining a function to convert X and Y from ngrams representation to embedddings representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    },
    "id": "Vjr6vwP5t8G9"
   },
   "outputs": [],
   "source": [
    "def convert_ngrams_to_embeddings(X_ngrams :List, Y_grams:List, vocabulary_index:Dict, word_embeddings:Dict):\n",
    "    \"\"\"This function is used to convert the provided ngrams into encoding representation \n",
    "    and for that, use the already created vocabulary_index dict and word embedding mappings.\n",
    "\n",
    "    Args:\n",
    "        X_ngrams (List): list of X ngrams\n",
    "        Y_grams (List): list of Y ngrams\n",
    "        vocabulary_index (Dict): vocab mapped to index\n",
    "        word_embeddings (Dict): word mapped to embedding\n",
    "\n",
    "    Returns:\n",
    "        X, Y: X and Y (having embedding representation)\n",
    "    \"\"\"\n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':[0....], ...}\n",
    "    # index to embedding : {1:[0....], ...} \n",
    "    # use your tokenizer's word_index to find the index of\n",
    "    # a given word\n",
    "    \n",
    "    #For X\n",
    "    X_sentences_embeddings = []\n",
    "    for items in X_ngrams:\n",
    "        X_sentence_embeddings = []\n",
    "        for index in items:\n",
    "            word = vocabulary_index[index]\n",
    "            embeddings = word_embeddings[word]\n",
    "            X_sentence_embeddings.extend(embeddings)\n",
    "        X_sentences_embeddings.append(X_sentence_embeddings)\n",
    "        \n",
    "    #For Y       \n",
    "    Y_total_embeddings = to_categorical(Y_grams)\n",
    "            \n",
    "    return X_sentences_embeddings, Y_total_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Converting X and Y from ngrams representation to embedddings representation from Spooky and Covid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step is computionally expensive. Set parameter in beginning to choose\n",
    "#which embeddings to use\n",
    "X_embeddings, Y_embeddings = convert_ngrams_to_embeddings(X, Y, vocab, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Defining a data generator for given X, Y and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    },
    "id": "H6g9g7p6t8G9"
   },
   "outputs": [],
   "source": [
    "NUM_SEQUENCES_PER_BATCH = 1280 # this is the batch size\n",
    "\n",
    "def data_generator(X_embeddings: List, y_embeddings: List, num_sequences_per_batch: int) -> (list,list):\n",
    "    \"\"\"Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels\n",
    "\n",
    "    Args:\n",
    "        X_embeddings (List): embeddings of X\n",
    "        y_embeddings (List): embeddings of y\n",
    "        num_sequences_per_batch (int): number of seqences per batch\n",
    "\n",
    "    Yields:\n",
    "        X, y: X and Y arrays with size of the batch\n",
    "    \"\"\"\n",
    "    steps_per_epochs = len(X_embeddings)//num_sequences_per_batch\n",
    "    current_step = 0\n",
    "    while True:\n",
    "        start_cell = (int(current_step % steps_per_epochs)) * num_sequences_per_batch\n",
    "        end_cell = (int(current_step % steps_per_epochs) + 1) * num_sequences_per_batch\n",
    "        yield np.array(X_embeddings[start_cell:end_cell]),np.array(y_embeddings[start_cell:end_cell])\n",
    "        current_step += 1\n",
    "        \n",
    "def create_generator(X_embeddings, Y_embeddings):\n",
    "    steps_per_epoch = len(X_embeddings)//NUM_SEQUENCES_PER_BATCH  # Number of batches per epoch\n",
    "    ngram_generator = data_generator(X_embeddings, Y_embeddings ,NUM_SEQUENCES_PER_BATCH)\n",
    "    sample = next(ngram_generator)\n",
    "    print(sample[0].shape)\n",
    "    print(sample[1].shape)\n",
    "\n",
    "    return ngram_generator, steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Creating a data generator for given X, Y and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "# steps_per_epoch = len(spooky_X_embeddings)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical\n",
    "\n",
    "# initialize data_generator\n",
    "ngram_generator, steps_per_epoch = create_generator(X_embeddings, Y_embeddings)\n",
    "# sample = next(ngram_generator)\n",
    "# print(sample[0].shape)\n",
    "# print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzfweqz1t8G-"
   },
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Defining model for building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture using Keras Sequential API\n",
    "def build_model(input_size:int, vocab_size:int):\n",
    "    \"\"\"This function is used to define architecture for the model\n",
    "\n",
    "    Args:\n",
    "        input_size (int): input size\n",
    "        vocab_size (int): vocab size for output\n",
    "\n",
    "    Returns:\n",
    "        Sequential: keras model\n",
    "    \"\"\"\n",
    "    # Define the model architecture using Keras Sequential API\n",
    "    model = Sequential()\n",
    "    model.add(Input((input_size, NGRAM*EMBEDDINGS_SIZE)))\n",
    "    model.add(Dense(NGRAM*EMBEDDINGS_SIZE, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    },
    "id": "KmgNnQj5t8G_"
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "model = build_model(len(X_embeddings), len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "id": "-dWf2qO3t8G_"
   },
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "print(\"Model Compiling\")\n",
    "model.compile(loss=CategoricalCrossentropy(), optimizer=SGD(learning_rate=.1), metrics=['accuracy'])\n",
    "print(\"Model Training\")\n",
    "model.fit(x=ngram_generator, steps_per_epoch=steps_per_epoch, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QwRhKYwt8HA"
   },
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Defining functions for sentence generation and word generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    },
    "id": "ewR5ueOJt8HB"
   },
   "outputs": [],
   "source": [
    "def generate_seq(seed: list, n_words: int) -> str:\n",
    "    \"\"\"This function is used to generate a seq of words for given model and tokenizer\n",
    "\n",
    "    Args:\n",
    "        seed (list): seed for sequence\n",
    "        n_words (int): number of words to be generated\n",
    "\n",
    "    Returns:\n",
    "        [str]: generated sentence\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    for i in range(0, n_words):\n",
    "        next_word = generate_next_word(seed)\n",
    "        sequence.append(next_word)\n",
    "        seed.append(next_word)\n",
    "        seed.pop(0)\n",
    "        # print(seed)\n",
    "    return \" \".join(sequence)\n",
    "\n",
    "def generate_next_word(seed: list) -> str:\n",
    "    \"\"\"This function is used to generate next word for given seed\n",
    "\n",
    "    Args:\n",
    "        seed (list): list of words [w1, w2, .., (n-1)]\n",
    "        \n",
    "    Returns:\n",
    "        str: generated word\n",
    "    \"\"\"\n",
    "    sentence_embeddings = []\n",
    "    array_embeddings = []\n",
    "\n",
    "    #generate initial set of words\n",
    "    for word in seed:\n",
    "        word_index = vocab_index_mapping[word]\n",
    "        word_embedding = wv[word_index]\n",
    "        # print(word_embedding)\n",
    "        sentence_embeddings.extend(word_embedding)\n",
    "    # print(len(sentence_embeddings))\n",
    "    array_embeddings.append(sentence_embeddings)\n",
    "    # print(len(array_embeddings))\n",
    "    array_embeddings = np.array(array_embeddings)\n",
    "    # print(array_embeddings.shape)\n",
    "    word_prob_matrix = model.predict(array_embeddings)\n",
    "    # print(word_prob_matrix)\n",
    "    next_word_index = np.random.choice(\n",
    "                        np.arange(len(word_prob_matrix[0])), 1, p=word_prob_matrix[0])[0]         \n",
    "    # word_index = np.where(word_prob_matrix == (np.max(word_prob_matrix)))\n",
    "    # print(word_index)\n",
    "    # print(word_index[0][0])\n",
    "    return vocab[next_word_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Generating sentences for a random seed sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of sentences\n",
    "\n",
    "# pick a random sample from the X as the initial seed\n",
    "random_sample_index_list = X[random.randint(0, len(X)-1)]\n",
    "seed = []\n",
    "for index in random_sample_index_list:\n",
    "    seed.append(vocab[index])\n",
    "# print(seed)\n",
    "\n",
    "for i in range(0, 50):\n",
    "    generated_sentence = generate_seq(seed, 20) \n",
    "    print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w68JVS2jt8HB"
   },
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE4dcQdut8HC"
   },
   "source": [
    "Answer the question: Do your neural language models produce n-grams that were not observed during training? (1 paragraph, you may support this answer with code as desired).\n",
    "\n",
    "- It can produce ngrams which are not observed during training. It is extremely unlikely to find a dataset which would cover all the possible n-grams that can be generated from the dataset. Lets say we consider n-gram \"the cat was\". For this, a possible ngram for these words could be \"was the cat\" which is something that may not be a part of the dataset. In that case, we will have a situation where we get a ngram that is not observed during training. If the words are close related to each other, in this instance, say \"was\", \"the\", \"cat\", then its possible that the model will generate a sequence with these words which is unseen in the training dataset. \n",
    "- Now, lets consider this scenario with numbers. Consider the sequence \"1 2 3 4 3 2 1\". Refering back to the naive bayes methodology of generating words in homework 2, given the initial character \"4\", the only number that could be generated from the matrix with probabilities would be \"3\". Now consider \"4 3\". After \"3\", the numbers \"2\" and \"4\" both appear after \"3\". Its possible to generate the sequence \"4 3 4\" given that \"3\" could appear after \"4\" and \"4\" could appear after \"3\". Relating this back to our word embeddings scenario, given a seed of \"4 3\", then model could predict the next value to be \"2\" or \"4\" because the embeddings should be similar in this scenario. \n",
    "- If we extend this idea to a much larger vocab and a model trained on many more sequences could generate a character n after (n-1) that has never appeared two spots after (n-2). Here n denotes the position of the character.\n",
    "\n",
    "<b>Compare your generated sentences: </b>\n",
    "\n",
    "- This neural language model can consider context information in a more sophisticated manner, however, the sentences generated that we observed made less sense. We should note our model's accuracy was quite low at approximately 9%. Neither sentences had good grammar, but the words following each other in the HW2 language model made some level of sense while the words generated after a seed in our neural model oftentimes appeared to be highly unrelated and make very little sense. \n",
    "- One example from the HW2 would be \"- \\<s> do they serve spicy \\</s>\". Compared to this example from our neural language model \"ploughed sufficient sidled jewelled garrets enacted toils both o capacity longue knot circumference then harbour weird felt honeycombed new bathed.\" This could be attributed to the relatively simple architecture of our model that couldn't transform the data much before classifying it. This dataset also seemed to be relatively small, but we don't have enough context to make that judgement. \n",
    "- While it's not clear from our data that neural language models have a better sense of context than the N_GRAM model, they can store much more information, which requires more data and a more thoughtful architecture to extract useful results.\n",
    "\n",
    "<b> Sentences generated from the neural language model using 100% of the spooky dataset: </b>\n",
    "\n",
    "- narrowed moat committee sister membrane look bitter parchment sun </s> distinguish crest felix jargon half endure simple like little </s>\n",
    "- </s> intervention unknowing winked landsmen fettered </s> aout by prosecute cud excesses unatteneded apparently but pollutes pitchy </s> scored essentially\n",
    "- air sympathised toll slayin egotism mace autobiography carting stormy tuberoses avert curator come communion purlieus sabbat </s> intruding toes right\n",
    "- extinction lowness latitude reckoned creeping kinsfolk bags damnable resurrection chords </s> insufficient authors scene light know </s> raise fortifications like\n",
    "- sensation effudit fatal inordinate africa way politician heard irish oldeb myself eyelid corner wide land god </s> seaport c away\n",
    "- depart s </s> alive awful </s> </s> </s> </s> overwhelmed cheek una scene fact </s> apartment hope men scapegrace there\n",
    "- mechanically wanton laboriously loft outlawed indifference hill recovering untraceable communication showed rowboat paper beau men orders apace resurrection ushers nigger\n",
    "- riddles all raymond instigated scandal anomalousness more villany </s> typifies lottery crammed froze stairs despair unfrequently despicable engendered inroads spirit\n",
    "- unknown equally lord armands thickened observer appeared walnut funerals caravan conquers stature elapsing piously ian unclothed whereto babel sharing misfortune\n",
    "- strange sun multitudinous account </s> trot drum joyful retired death knees one vast twelve buoyantly print groped park inquietude places\n",
    "- billows financial void pitchers carcass antiphonal zest inconvenient </s> earnest lathered however </s> rehoboth </s> </s> exergues it shifts op\n",
    "- foremost refresh subtle decorated frail islands conciliatory resurgence described patriotic aeronaut exploring avenger chemist slips presence transmitting shuns semi excessive\n",
    "- ship convicted senatorial obliterating eend vaow caked divest dark welcome alkaloid noxious loftily prophecies small observation partly morbid steeds seizure\n",
    "- turns monstrosity slovenly senct beguiled favourites gates guinea bristol unflinching spiritual puppy annihilate use multiplicity glaciers frank displace remained subject\n",
    "- liveliest exhalations unspeakable infection gossiped constellations species arabesque till grounds simple </s> </s> came imminence unsought steeply topical headquarters gloss\n",
    "- rally dispatched bon aëroplane cautioned greedy small mr arduous syllable adams masculine soften bartholinus representatives infidels eluding gilman degenerating heather\n",
    "- towering alexandre wan mistiness civilisation bestowed athenians foiled farther disillusion marie rummer chuckle drains scholar execrate pleasureable denied queerness strides\n",
    "- elephant foresight recuperating tray caude refining vengeful bellows pictures subserved ruins green shock unreserved me death </s> hands said ketch\n",
    "- wheezed mercantile hideous elihu course antennæ absolutely </s> i different wig </s> thing country </s> head strictness huddled main embrasure\n",
    "- years meanest hollands poorhouse disjointedly es flavius maurice ignominious angelic water preignac cloud story eccentrick </s> animadversions pea cyrus swammerdamm\n",
    "- stick denominated dents invulnerable risers cultivate bluddennuff delighted massacring smug answer interred calmed alcyone conversation overturning use stealing contradictions irrefutable\n",
    "- trammpled grief sowing obstrusive polite jostled puff ongas shelterless triumphs meaning unfastened drawer heated holmes venetian indulges retarded o bafflingly\n",
    "- lowness factitious mankind economy untamed de elagabalus nerve animating dyin volcanoes carries grouping soared idlers unmixed purse obadiah engine overran\n",
    "- aid mithridates tempered rubbish darkness untimely helvia curiosity crisp violate and bruise outstretched disputing offend quieted opponents proffit irrelevant affair\n",
    "- directing </s> directionless right grew assisting glorified murmuring distinctly bribing sister blunting decomposition happiness dignity </s> countenance years dwelled introduce\n",
    "- adversities tastes afflatus educations hallowmass longer veil substantive dully hanging fig succourless rest estranged rest disputed merciful murmur countenance hope\n",
    "- </s> me wonderful white topical vernunft exterminating energetically old </s> sea speak </s> forward furnaces dread whitewash physiognomy apothecary argument\n",
    "- ideal countenance knows intelligence oaken lobe draughts merrival dieu famed esteban ramble satisfactorily existence analyzed lobster humans fanciful frayed informe\n",
    "- gemmary shepherd exceedingly projected dawn xura worthy rhine magnificence sky furtively artistic west seemed other approached souvenir misery sartain kingdom\n",
    "- </s> love hardly throws like abhorrence understood target hardens wants disfigured lithe reverent emphatically empires cameleopards orontes horses class daddyship\n",
    "- delved eventful terming weird cough ignorance travels malignancy tool persuasions lh languidly composed manners gnorri repute amorphous society berlifitzing morning\n",
    "- says conquering </s> evenly second attention fishermen way horror exhibit attention met </s> person exclude relish plumped merits visibility america\n",
    "- desires republican know lapse untrue encampment induce ut light gone wiry servitude critter stretch computed unus dross clearly air looking\n",
    "- muttering been evidence water room </s> harangue dangers turk wall directions thing open extravagances necessarily telescopes profuseness possible evidently smote\n",
    "- devotions scourged tortures longer signs distributing nill animation apotheosis trotter pesty italian peg dissimulation unmingled natural brimmed me bargeman runnin\n",
    "- decreases unconsciousness daedalus thy innovation fortified hogsheads slovenly rate discussions run acted abstemious peradventure exuberant frivolous tampering relaxation mythology worthlessness\n",
    "- sidereal brusquerie proved harmonized paralyzing forth stranger believe </s> arkham scholarly seamen innumerable preferably immolation gesticulations striven flux robber post\n",
    "- firmness thar men procure masterminds irregularity </s> introduction </s> darkness it unhuman air stupendous damsel polynices culled notoriously cenis resolve\n",
    "- shunned an flight artemis maillardet moody knew fiend theorize halo comfortless trotter interferin antagonist lustre robust leads unused romance vascones\n",
    "- </s> energy torrent fungous which power spirit venny it recognition tressel overnourished dark gay ascension decision soother habitués stupid enterred\n",
    "- pumps flos reproduce cannibal flame green diavolo spot before sime sublunary cicale tantalized binding administration retreating </s> sufficient hatred dream\n",
    "- man intrigue maps looked bristol inheritance wall instil wines unhappily fervour alarmedly doubles anecdotes embellished haggardness alloy stopping height springs\n",
    "- ploughed sufficient sidled jewelled garrets enacted toils both o capacity longue knot circumference then harbour weird felt honeycombed new bathed\n",
    "- slaying diagnosed proposed lonely untenanted deficient daemonologist domestic grey donnelly opiates care diet illogically voice petto tantalisingly crucifix turk government\n",
    "- fences essayed palliative ruts threw correcting converse attention summoning rebuke imagining ridicule zaiat imprisonment coast account parthenon inattentive words verbatim\n",
    "- befitting like cureless overheard younguns started dwelling like me infernal faded object horrific remoter swept misapplied collected things knowed seaweed\n",
    "- area nova protection focussing lost même sixteenth portent naivete luxuriant muddy stupid altering disinterested abaft maple nourished waiting dulls mad\n",
    "- trabes placing learn nature sorrow civilian maintained </s> better monster z </s> stare combined sentiments delight hands twilights alas flight\n",
    "- expectant </s> </s> embrace r identified staggering cincinnatus tones place crying associations selfishness half polynesian reasons hxmx submarine occupying suffered\n",
    "- empire </s> let prosy lived clear square strange austrians plotted lower superintended prayers ver solely night harpies felt greedy superadded\n",
    "\n",
    "<b> Sentences generated with Jinesh Language Model from HW2 </b>\n",
    "\n",
    "Model: bigram, laplace smoothed\n",
    "Sentences:\n",
    "\n",
    "(Removing sentence start and end tokens because they are causing display issues in HTML file)\n",
    "\n",
    "- ten and twenty dollars \n",
    "- any polish food \n",
    "- it \n",
    "- it on martin luther king avenue \n",
    "- do they serve spicy \n",
    "- forget it doesn't matter \n",
    "- around icsi \n",
    "- it should be sunday \n",
    "- five dollars \n",
    "- just two miles give me more than a few minute \n",
    "- i would like to spend less than a mile \n",
    "- about christopher's cafe \n",
    "- what is the list please \n",
    "- okay \n",
    "- not say about restoran-rasa-sayang \n",
    "- now i'd like to eat some more about monday \n",
    "- please give me about tuesday \n",
    "- i would be any like to icsi \n",
    "- i wanna go out to icsi \n",
    "- i wanna burrito \n",
    "- i see the a uh which of hong-kong villa international house \n",
    "- please give me information about great china \n",
    "- give me a soup do you have any day \n",
    "- the distance \n",
    "- i want to eat indian restaurants that question \n",
    "- where can i want to spend fifteen dollars \n",
    "- i don't care how about guerrero's \n",
    "- i'm not say uh to a car so it should be inexpensive \n",
    "- okay now \n",
    "- i'm looking for a thai barbecue flint's barbecue the service at pasand \n",
    "- uh thai food i'd like to eat sushi \n",
    "- chicken \n",
    "- uh find me about restaurants \n",
    "- start over \n",
    "- i like to find a sunday \n",
    "- i want to eat breakfast \n",
    "- i can you get dinner \n",
    "- i want to eat uh i'm looking for dessert \n",
    "- dinner thursday \n",
    "- what i would a cheap cafe \n",
    "- not on shattuck \n",
    "- start over \n",
    "- i have additional information about any day \n",
    "- so it can be reasonably cheap \n",
    "- start over \n",
    "- next thursday or however you have some indian food in <UNK> asian \n",
    "- indian food would like to find out for lunch \n",
    "- what kind of the <UNK> \n",
    "- show me the cost at places "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yet5p8N1t8HC"
   },
   "source": [
    "References Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "2. https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/\n",
    "3. https://blog.paperspace.com/pre-trained-word-embeddings-natural-language-processing/\n",
    "4. https://hemantranvir.medium.com/spam-detection-using-rnn-simplernn-lstm-with-step-by-step-explanation-530367608071\n",
    "5. https://www.tensorflow.org/text/guide/word_embeddings\n",
    "6. https://radimrehurek.com/gensim/models/word2vec.html\n",
    "7. https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wordembeddings_starter.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
