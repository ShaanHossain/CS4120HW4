{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjJn2R-Ft8Gi"
   },
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: __Shaan Hossain, Jinesh Shailesh Mehta__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwlPkjgt8Gq"
   },
   "source": [
    "Step 1: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "(describe the provided dataset that you have chosen here)\n",
    "\n",
    "Describe what data set you have chosen to compare and contrast with the your chosen provided dataset. Make sure to describe where it comes from and it's general properties.\n",
    "\n",
    "(describe your dataset here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    },
    "id": "uQLg8dGdt8Gr"
   },
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "\n",
    "from typing import List\n",
    "from gensim.parsing.preprocessing import stem_text, remove_stopwords, strip_punctuation\n",
    "import keras\n",
    "import numpy \n",
    "import pandas\n",
    "import scipy\n",
    "import sklearn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from csv import reader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj0A0mCkt8Gt"
   },
   "source": [
    "### a) Train embeddings on GIVEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO: Jinesh\\n1. Parsing the custom dataset\\n2. Pre processing function - Completed.\\n\\nparse_data -> preprocessing ->\\n\\nTODO: Shaan\\n1. Get the infrastructure for the model set up to accept data from preprocessing\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO: Questions to submit\n",
    "1. Do we need to worry about the training time? Can we submit a pretrained model\n",
    "or is part of training what you want to see.\n",
    "2. Do we need to train on both of the datasets\n",
    "'''\n",
    "\n",
    "'''\n",
    "NOTE: For pre processing\n",
    "1. lower\n",
    "2. remove punctuation\n",
    "3. remove stopwords\n",
    "4. Add <s> and </s> for every sentence.\n",
    "'''\n",
    "\n",
    "'''\n",
    "TODO: Jinesh\n",
    "1. Parsing the custom dataset\n",
    "2. Pre processing function - Completed.\n",
    "\n",
    "parse_data -> preprocessing ->\n",
    "\n",
    "TODO: Shaan\n",
    "1. Get the infrastructure for the model set up to accept data from preprocessing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:25.438770Z",
     "start_time": "2020-10-24T04:39:24.888507Z"
    },
    "id": "x2IJbX_Mt8Gu"
   },
   "outputs": [],
   "source": [
    "# code to train your word embeddings\n",
    "\n",
    "# Read the file and prepare the training data \n",
    "# so that it is in the following format\n",
    "\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "def parse_data(training_file_path: str, percentage: int, select_column:int) -> List[str]:\n",
    "  \"\"\"This function is used to parse input lines\n",
    "  and returns a the provided percent of data.\n",
    "\n",
    "  Args:\n",
    "      lines (List[str]): list of lines\n",
    "      percentage (int): percent of the dataset needed\n",
    "      select_column (int): column to be selected from the dataset\n",
    "  Returns:\n",
    "      List[str]: lines (percentage of dataset)\n",
    "  \"\"\"\n",
    "  sentences = []\n",
    "  percentage_sentences = []\n",
    "  with open(training_file_path, \"r\", encoding=\"utf8\", errors=\"ignore\") as csvfile:\n",
    "    csv_reader = reader(csvfile)\n",
    "\n",
    "    # csv_reader_copy = csv_reader.copy()\n",
    "\n",
    "    #skipping header\n",
    "    header = next(csv_reader)\n",
    "\n",
    "    # line_length = len(list(csv_reader_copy))\n",
    "   \n",
    "    if header != None:\n",
    "      for row in csv_reader:\n",
    "        sentences.append(row[select_column])\n",
    "\n",
    "    end_of_data = int(len(sentences) * percentage * .01)\n",
    "    percentage_sentences = sentences[0:end_of_data]\n",
    "\n",
    "  return percentage_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"I am sam. Sam I am. What time is it?\" \n",
    "# -> \n",
    "# List of sentences [<s> I am sam </s>, <s> Sam I am </s> ...]\n",
    "\n",
    "def preprocessing(running_lines: List[str]) -> List[List[str]]:\n",
    "  \"\"\"This function takes in the running test and return back the\n",
    "  preprocessed text. Four tasks are done as part of this:\n",
    "    1. lower word case\n",
    "    2. remove stopwords\n",
    "    3. remove punctuation\n",
    "    4. Add <s> and </s> for every sentence\n",
    "\n",
    "  Args:\n",
    "      running_lines (List[str]): list of lines\n",
    "\n",
    "  Returns:\n",
    "      List[List[str]]: list of sentences where each sentence is broken\n",
    "                        into list of words.\n",
    "  \"\"\"\n",
    "  preprocessed_lines = []\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "  for line in running_lines:\n",
    "    lower_case_data = line.lower()\n",
    "    data_without_stop_word = remove_stopwords(lower_case_data)\n",
    "    data_without_punct = strip_punctuation(data_without_stop_word)\n",
    "    processed_data = tokenizer.tokenize(data_without_punct)\n",
    "    processed_data.insert(0,\"<s>\")\n",
    "    processed_data.append(\"</s>\")\n",
    "    preprocessed_lines.append(processed_data)\n",
    "  return preprocessed_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Provided Dataset: Spooky Dataset\n",
      "\n",
      "['<s>', 'process', 'however', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', 'circuit', 'return', 'point', 'set', 'out', 'aware', 'fact', 'perfectly', 'uniform', 'wall', '</s>']\n",
      "['<s>', 'occurred', 'fumbling', 'mere', 'mistake', '</s>']\n",
      "['<s>', 'left', 'hand', 'gold', 'snuff', 'box', 'which', 'capered', 'hill', 'cutting', 'manner', 'fantastic', 'steps', 'took', 'snuff', 'incessantly', 'air', 'greatest', 'possible', 'self', 'satisfaction', '</s>']\n",
      "['<s>', 'lovely', 'spring', 'looked', 'windsor', 'terrace', 'sixteen', 'fertile', 'counties', 'spread', 'beneath', 'speckled', 'happy', 'cottages', 'wealthier', 'towns', 'looked', 'years', 'heart', 'cheering', 'fair', '</s>']\n",
      "['<s>', 'finding', 'else', 'gold', 'superintendent', 'abandoned', 'attempts', 'perplexed', 'look', 'occasionally', 'steals', 'countenance', 'sits', 'thinking', 'desk', '</s>']\n",
      "['<s>', 'youth', 'passed', 'solitude', 'best', 'years', 'spent', 'gentle', 'feminine', 'fosterage', 'refined', 'groundwork', 'character', 'overcome', 'intense', 'distaste', 'usual', 'brutality', 'exercised', 'board', 'ship', 'believed', 'necessary', 'heard', 'mariner', 'equally', 'noted', 'kindliness', 'heart', 'respect', 'obedience', 'paid', 'crew', 'felt', 'peculiarly', 'fortunate', 'able', 'secure', 'services', '</s>']\n",
      "['<s>', 'astronomer', 'perhaps', 'point', 'took', 'refuge', 'suggestion', 'non', 'luminosity', 'analogy', 'suddenly', 'let', 'fall', '</s>']\n",
      "['<s>', 'surcingle', 'hung', 'ribands', 'body', '</s>']\n",
      "['<s>', 'knew', 'stereotomy', 'brought', 'think', 'atomies', 'theories', 'epicurus', 'since', 'discussed', 'subject', 'long', 'ago', 'mentioned', 'singularly', 'little', 'notice', 'vague', 'guesses', 'noble', 'greek', 'met', 'confirmation', 'late', 'nebular', 'cosmogony', 'felt', 'avoid', 'casting', 'eyes', 'upward', 'great', 'nebula', 'orion', 'certainly', 'expected', 'so', '</s>']\n",
      "\n",
      "Tokenizing Custom Dataset: Corona Tweets\n",
      "\n",
      "['<s>', 'menyrbie', 'phil', 'gahan', 'chrisitv', 'https', 't', 'co', 'ifz9fan2pa', 'https', 't', 'co', 'xx6ghgfzcc', 'https', 't', 'co', 'i2nlzdxno8', '</s>']\n",
      "['<s>', 'advice', 'talk', 'neighbours', 'family', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'phone', 'numbers', 'neighbours', 'schools', 'employer', 'chemist', 'gp', 'set', 'online', 'shopping', 'accounts', 'poss', 'adequate', 'supplies', 'regular', 'meds', 'order', '</s>']\n",
      "['<s>', 'coronavirus', 'australia', 'woolworths', 'elderly', 'disabled', 'dedicated', 'shopping', 'hours', 'amid', 'covid', '19', 'outbreak', 'https', 't', 'co', 'binca9vp8p', '</s>']\n",
      "['<s>', 'food', 'stock', 'empty', 'please', 'don', 't', 'panic', 'food', 'need', 'stay', 'calm', 'stay', 'safe', 'covid19france', 'covid', '19', 'covid19', 'coronavirus', 'confinement', 'confinementotal', 'confinementgeneral', 'https', 't', 'co', 'zrlg0z520j', '</s>']\n",
      "['<s>', 'me', 'ready', 'supermarket', 'covid19', 'outbreak', 'i', 'm', 'paranoid', 'food', 'stock', 'litteraly', 'empty', 'coronavirus', 'thing', 'please', 'don', 't', 'panic', 'causes', 'shortage', 'coronavirusfrance', 'restezchezvous', 'stayathome', 'confinement', 'https', 't', 'co', 'usmualq72n', '</s>']\n",
      "['<s>', 'news', 'region', 's', 'confirmed', 'covid', '19', 'case', 'came', 'sullivan', 'county', 'week', 'people', 'flocked', 'area', 'stores', 'purchase', 'cleaning', 'supplies', 'hand', 'sanitizer', 'food', 'toilet', 'paper', 'goods', 'tim', 'dodson', 'reports', 'https', 't', 'co', 'cfxch7a2lu', '</s>']\n",
      "['<s>', 'cashier', 'grocery', 'store', 'sharing', 'insights', 'covid', '19', 'prove', 'credibility', 'commented', 'i', 'm', 'civics', 'class', 'know', 'i', 'm', 'talking', 'about', 'https', 't', 'co', 'iefdnehgdo', '</s>']\n",
      "['<s>', 'supermarket', 'today', 'didn', 't', 'buy', 'toilet', 'paper', 'rebel', 'toiletpapercrisis', 'covid', '19', 'https', 't', 'co', 'evxkqlidaz', '</s>']\n",
      "['<s>', 'covid', '19', 'retail', 'store', 'classroom', 'atlanta', 'open', 'walk', 'in', 'business', 'classes', 'weeks', 'beginning', 'monday', 'march', '16', 'continue', 'process', 'online', 'phone', 'orders', 'normal', 'thank', 'understanding', 'https', 't', 'co', 'kw91zj5o5i', '</s>']\n",
      "['<s>', 'corona', 'prevention', 'we', 'stop', 'buy', 'things', 'cash', 'use', 'online', 'payment', 'methods', 'corona', 'spread', 'notes', 'prefer', 'online', 'shopping', 'home', 'it', 's', 'time', 'fight', 'covid', '19', 'govindia', 'indiafightscorona', '</s>']\n",
      "['<s>', 'month', 'hasn', 't', 'crowding', 'supermarkets', 'restaurants', 'reducing', 'hours', 'closing', 'malls', 'means', 'entrance', 'dependent', 'single', 'supermarket', 'manila', 'lockdown', 'covid2019', 'philippines', 'https', 't', 'co', 'hxws9lanf9', '</s>']\n",
      "['<s>', 'covid', '19', 'situation', 'increased', 'demand', 'food', 'products', 'wait', 'time', 'longer', 'online', 'orders', 'particularly', 'beef', 'share', 'freezer', 'packs', 'thank', 'patience', 'time', '</s>']\n",
      "['<s>', 'horningsea', 'caring', 'community', 'let', 's', 'look', 'capable', 'village', 'ensure', 'stay', 'healthy', 'bringing', 'shopping', 'doors', 'help', 'online', 'shopping', 'self', 'isolation', 'symptoms', 'exposed', 'somebody', 'has', 'https', 't', 'co', 'lsgrxxhjhh', '</s>']\n",
      "['<s>', 'me', 'don', 't', 'need', 'stock', 'food', 'i', 'll', 'amazon', 'deliver', 'need', 'coronavirus', 'amazon', 'https', 't', 'co', '8ywakfjexc', '</s>']\n",
      "['<s>', 'adara', 'releases', 'covid', '19', 'resource', 'center', 'travel', 'brands', 'insights', 'help', 'travel', 'brands', 'stay', 'up', 'to', 'date', 'consumer', 'travel', 'behavior', 'trends', 'https', 't', 'co', 'pna797jdkv', 'https', 't', 'co', 'dqox6usihz', '</s>']\n",
      "['<s>', 'lines', 'grocery', 'store', 'unpredictable', 'eating', 'safe', 'alternative', 'avoiding', 'restaurants', 'right', 'now', 'https', 't', 'co', '9idzsis5oq', 'coronavirus', 'covid19', 'https', 't', 'co', 'zhbh898lf6', '</s>']\n",
      "['<s>', '13', 'https', 't', 'co', '51bl8p6vzh', '</s>']\n",
      "['<s>', 'eyeonthearctic', '16mar20', 'russia', 'consumer', 'surveillance', 'watchdog', 'reported', 'case', 'high', 'arctic', 'man', 'traveled', 'iran', 'covid', '19', '101', 'observed', 'https', 't', 'co', '4wnrrk9okc', 'https', 't', 'co', 'ld05k5eyns', '</s>']\n",
      "['<s>', 'amazon', 'glitch', 'stymies', 'foods', 'fresh', 'grocery', 'deliveries', 'as', 'covid', '19', 'spread', 'we', 've', 'seen', 'significant', 'increase', 'people', 'shopping', 'online', 'groceries', 'spokeswoman', 'said', 'statement', 'today', 'resulted', 'systems', 'impact', 'affecting', 'https', 't', 'co', 'tbzz2mc3b3', '</s>']\n",
      "['<s>', 'aren', 't', 'struggling', 'consider', 'donating', 'food', 'bank', 'nonprofit', 'demand', 'services', 'increase', 'covid', '19', 'impacts', 'jobs', 'people', 's', 'way', 'life', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# parse and preprocess provided data\n",
    "print(\"Tokenizing Provided Dataset: Spooky Dataset\\n\")\n",
    "tokenized_sentences = preprocessing(parse_data(\"data\\\\provided\\\\train\\\\train.csv\", .05, 1))\n",
    "for token in tokenized_sentences:\n",
    "  print(token)\n",
    "# parse and preprocess custom data\n",
    "print(\"\\nTokenizing Custom Dataset: Corona Tweets\\n\")\n",
    "tokenized_sentences = preprocessing(parse_data(\"data\\custom\\\\train\\\\Corona_NLP_train.csv\", .05, 4))\n",
    "for token in tokenized_sentences:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-1.4 Choosing the dataset:\n",
    "Coronavirus tweets NLP\n",
    "https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    },
    "id": "Od_L53GEt8Gv"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "# sg = 1\n",
    "# window = 5\n",
    "# size = EMBEDDINGS_SIZE\n",
    "# min_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:43.448249Z",
     "start_time": "2020-10-24T04:39:43.444835Z"
    },
    "id": "xrt52ahnt8Gw"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11212/1233419857.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# if you save your Word2Vec as the variable model, this will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# print out the vocabulary size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Vocab size {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "print('Vocab size {}'.format(len(model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    },
    "id": "UUanXgQLt8Gy"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "# model.wv.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGMUTMcmt8G0"
   },
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN0KvmUKt8G0"
   },
   "outputs": [],
   "source": [
    "# then do a second data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsjzTVFjt8G1"
   },
   "source": [
    "What text-normalization and pre-processing did you do and why? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOFmHpH8t8G2"
   },
   "source": [
    "Step 2: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXjy2-OqgvIf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWlWydbrgv4P"
   },
   "source": [
    "##Write down your analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tmrTVDqt8G2"
   },
   "source": [
    "Cite your sources:\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix2On6zat8G2"
   },
   "source": [
    "Step 3: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZsCKQWDt8G2"
   },
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    },
    "id": "ec0KKYj0t8G3"
   },
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# The size of the ngram language model you want to train\n",
    "# change as needed for your experiments\n",
    "NGRAM = 3 \n",
    "\n",
    "# Initializing a Tokenizer\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:54.373208Z",
     "start_time": "2020-10-24T03:27:54.369835Z"
    },
    "id": "U1PrwlBAt8G5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCndArPmt8G5"
   },
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jG42_9Xt8G6"
   },
   "source": [
    "#### Fixed ngram based sequences "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HsoPVS8ct8G7"
   },
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process               however\n",
    "    process, however               afforded\n",
    "    however, afforded\t           me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    },
    "id": "B_4YqhKTt8G7"
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(ngram: list) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWL6Czlxt8G8"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:31.213422Z",
     "start_time": "2020-10-24T05:21:31.061759Z"
    },
    "id": "csweN-d1t8G9"
   },
   "outputs": [],
   "source": [
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    },
    "id": "Vjr6vwP5t8G9"
   },
   "outputs": [],
   "source": [
    "def read_embeddings():\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    '''\n",
    "    \n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':[0....], ...}\n",
    "    # index to embedding : {1:[0....], ...} \n",
    "    # use your tokenizer's word_index to find the index of\n",
    "    # a given word\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    },
    "id": "H6g9g7p6t8G9"
   },
   "outputs": [],
   "source": [
    "def data_generator(X: list, y: list, num_sequences_per_batch: int) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:55.470133Z",
     "start_time": "2020-10-24T05:22:55.398259Z"
    },
    "id": "vgXSWdlMt8G-"
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "# initialize data_generator\n",
    "# num_sequences_per_batch = 128 # this is the batch size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzfweqz1t8G-"
   },
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:56:19.207252Z",
     "start_time": "2020-10-24T04:56:19.204894Z"
    },
    "id": "4fZlHukVt8G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    },
    "id": "KmgNnQj5t8G_"
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "\n",
    "# Define the model architecture using Keras Sequential API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "id": "-dWf2qO3t8G_"
   },
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "model.fit(x=train_generator, \n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVjtknkVt8HA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCZ2S5mpt8HA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QwRhKYwt8HA"
   },
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    },
    "id": "ewR5ueOJt8HB"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list, \n",
    "                 n_words: int):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:14:13.123529Z",
     "start_time": "2020-10-24T04:14:13.000264Z"
    },
    "id": "XZ9fShSyt8HB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w68JVS2jt8HB"
   },
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE4dcQdut8HC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yet5p8N1t8HC"
   },
   "source": [
    "Sources Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sPu_1h2t8HC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wordembeddings_starter.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
